---
title: Transformer - Encoder
description: ë³¸ ê¸€ì€ cs224në¥¼ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤. 
slug: llm
date: 2025-06-22 15:45:00+0900
categories:
    - LLM
tags:
    - LLM
    - AI
weight: 1  # You can add weight to some posts to override the default sorting (date descending)
---

[##_Image|kage@ts3IR/btsOMTqkBt1/AAAAAAAAAAAAAAAAAAAAAO5eCa8c7NYTnezb7sEzqKBacKSPresrBDlEMbGfNoLD/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=W679xjbXXwiqbkktqAK99VZUDmE%3D|CDM|1.3|{"originWidth":602,"originHeight":667,"style":"alignCenter","width":425,"height":471}_##]

Transformer DecoderëŠ” **language modelì²˜ëŸ¼ ë‹¨ë°©í–¥(unidirectional) contextë§Œì„ ì‚¬ìš©í•˜ë„ë¡ contraint(ì œí•œ)**í•œë‹¤.Â 

\-> ì¦‰ ê³¼ê±°(ì™¼ìª½) ì •ë³´ë§Œ ë³´ê³  ë¯¸ë˜(ì˜¤ë¥¸ìª½)ëŠ” ë³´ì§€ ì•ŠëŠ”ë‹¤.Â 

\-> ì´ëŠ” Langue modelì²˜ëŸ¼ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” task

ê·¸ëŸ¬ë©´ ìš°ë¦¬ëŠ” **bidirectional contextë¥¼ ì›í•˜ë©´ ì–‘ë°©í–¥ RNNì²˜ëŸ¼ í•˜ë©´ ë ê¹Œ?**Â 

ê·¸ëŸ´ ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´!!!!! **Transformer Encoder**ì´ë‹¤!

\-> ì´ëŠ” ë¬¸ì¥ ì´í•´, ë¶„ë¥˜ì— ì í•©í•œ taskÂ 

**Decoderì™€ì˜ ìœ ì¼í•œ ì°¨ì´ì ì€ self-attentionì—ì„œ maskingì„ ì œê±°í•œë‹¤ëŠ” ê²ƒ..! (ì¦‰, ëª¨ë“  ë‹¨ì–´ë¥¼ ë™ì‹œì— ë³¸ë‹¤!)**Â 

### **The transformer Encoder-Decoder**

[##_Image|kage@EsUBF/btsOMe2P77d/AAAAAAAAAAAAAAAAAAAAAMMZMPLAgERAICX1MeRLDaUvqrufJ3FCTwJ4Acj8HCpg/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=lSVxZmTQSeho1FJ6UDsGGzTTBM0%3D|CDM|1.3|{"originWidth":648,"originHeight":694,"style":"alignCenter","width":381,"height":408}_##]

Machine translationì„ íšŒê¸°í•˜ë©´,,Â 

ìš°ë¦¬ëŠ” bidirectional modelì—ì„œ source sentenceë¥¼ ì²˜ë¦¬í–ˆê³ , unidirectional modelì—ì„œ targetì„ ìƒì„±í–ˆë‹¤!Â Â 

seq2seq formatì˜ ì‘ì—…ì—ì„œëŠ”... ì´ë ‡ê²Œ Transformer Encoder-Decoder êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤!!!!!

-   **Encoder**  
    ì¼ë°˜ì ì¸ Transformer Encoderë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
-   **Decoder**  
    Encoderì˜ ì¶œë ¥ì— ëŒ€í•´ **cross-attention**ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •ëœë‹¤. DecoderëŠ” **self-attention**ìœ¼ë¡œ ìê¸° ìì‹ ë§Œ ë³´ëŠ”ê²Œ ì•„ë‹ˆë¼, Encoderê°€ ë§Œë“  input ë¬¸ì¥ì˜ contextë¥¼ ì°¸ê³ í•´ì„œ ë²ˆì—­ì„ ìƒì„±Â 

### **Cross-attention (details)**

[##_Image|kage@c0gxt1/btsOMku0Kfk/AAAAAAAAAAAAAAAAAAAAABG7eNvJgk9_dlDwwjsrY0PNbkeQG-hXvdGsIMMsGfdq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=Wz7T1TPA5m%2BSrkRUCGrk21%2FpPg4%3D|CDM|1.3|{"originWidth":597,"originHeight":482,"style":"alignCenter","width":382,"height":308}_##]

**self-attentionì€ ê°™ì€ sourceì—ì„œ keys, queries, valuesê°€ ë§Œë“¤ì–´ì§ˆ ë•Œë¥¼ ë§í•œë‹¤.**

**ì¦‰, Encoder ë‚´ë¶€, Decoder ë‚´ë¶€**Â 

Decoderì—ì„œëŠ” self-attentionë„ ìˆì§€ë§Œ, ë‹¤ë¥¸ sourceë¥¼ ë°”ë¼ë³´ëŠ” attentionë„ ìˆë‹¤!(ì €ë²ˆ ì‹œê°„ì— ë°°ì›€) ê·¸ê²Œ ë°”ë¡œ **cross-attention**

-   **h1, ... , hn**: Transformer **encoder** ë¡œ ë¶€í„° ì–»ì€ **output vector(hidden state)**Â ğ‘¥ğ‘–âˆˆâ„ğ‘‘
-   **z1, ... , zn**: Transformer **decoder**ë¡œ ë¶€í„° ì–»ì€ **input vector(ì¦‰, ì´ë¯¸ ìƒì„±ëœ ë‹¨ì–´ë“¤ì˜ hidden state)**Â ğ‘§ğ‘–âˆˆâ„ğ‘‘

ì—¬ê¸°ì„œ keysì™€ valuesëŠ” encoderì˜ ì¶œë ¥ hiì—ì„œ ê³„ì‚°

ì¦‰ EncoderëŠ” ì¼ì¢…ì˜ memory ê°™ì€ ëŠë‚ŒÂ 

ğ‘˜ğ‘–=ğ¾â„ğ‘–,Â ğ‘£ğ‘–=ğ‘‰â„ğ‘–

queriesëŠ” decoderë¡œ í˜„ì¬ ì…ë ¥ ziì—ì„œ ê°€ì ¸ì˜¨ ê²ƒÂ 

ğ‘ğ‘–=ğ‘„ğ‘§ğ‘–.

ê·¸ë‹ˆê¹Œ ì •ë¦¬í•˜ìë©´... queryëŠ” decoderì˜ input ìš°ë¦¬ê°€ ì§‘ì¤‘í•´ì„œ ë³´ê³  ì‹¶ì€ ê²ƒë“¤ì€ encoderì˜ inputì´ê³ , keyì™€ valueì— í•´ë‹¹Â 

ì¦‰ Decoderê°€ Encoderì˜ ì¶œë ¥ì— ëŒ€í•´ attention ì„ ë‚ ë¦¬ëŠ” ê²ƒ! **ì°¸ì¡°í•˜ë©° ë¬¸ì¥ì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ!!**

### Cross-attentionì´ ê³„ì‚°ë˜ëŠ” ê³¼ì •Â 

[##_Image|kage@leV61/btsOLl2zPW4/AAAAAAAAAAAAAAAAAAAAAIA7cXyO2OQFSby4v8-pcRp2XmxzWFnaRyUMSH9ihgPv/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=V2xFp9KXOuxlWZ8yLjJQZxeZ4EI%3D|CDM|1.3|{"originWidth":309,"originHeight":88,"style":"alignCenter"}_##]

HëŠ” encoder vectorì˜ concatenaton!Â 

ZëŠ” dcoder vectorì˜ concatenation!

ì—¬ê¸°ì„œ TëŠ” encoderì˜ ê¸¸ì´ ë˜ëŠ” decoderì˜ ê¸¸ì´ ì¤‘ í•˜ë‚˜ì¼ ìˆ˜ ìˆëŠ”ë°, **ë¬¸ë§¥ìƒ ë‘˜ ë‹¤ Të¡œ í‘œê¸°í•œ ê±°ì§€ ì‹¤ì œë¡œëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆì–´.**

-   ì˜ˆë¥¼ ë“¤ì–´: ì˜ì–´ ì…ë ¥(5ë‹¨ì–´) â†’ í”„ë‘ìŠ¤ì–´ ì¶œë ¥(7ë‹¨ì–´)ì´ë¼ë©´
    -   Encoder: Tâ‚ = 5, Decoder: Tâ‚‚ = 7

[##_Image|kage@cgCHrb/btsOMFyVJLj/AAAAAAAAAAAAAAAAAAAAAAfCTekS-Os5iVz3UuE7abPQNnIuvyBsZc7hVMY0M0_8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=YXRMdc7KiYsDXz9NJeiVHIeoqbY%3D|CDM|1.3|{"originWidth":470,"originHeight":43,"style":"alignCenter"}_##]

decoder vectorì— Queryë¥¼ ê³±í•˜ê³ Â 

encoder vectorì— Keyë¥¼ ê³±í•œ í›„Â 

ê·¸ ë‘˜ì„ ê³„ì‚°í•˜ì—¬ Attention scoreë¥¼ ê³„ì‚° ì´ë¥¼ softmaxë¡œ í†µê³¼í•˜ì—¬ attention distribution ì¦‰, ê°€ì¤‘ì¹˜ë¥¼ ë§Œë“¤ì–´ ë‚´ê³ Â 

ì´ë¥¼ encoder vectorì˜ valueë“¤ì— ê³±í•œë‹¤!!Â 

[##_Image|kage@bWv0Qu/btsOMcjF5EO/AAAAAAAAAAAAAAAAAAAAAAhMLglNh1dZVXt4HF_o3G49dac-4pEB-YMPtuY9VRzM/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=zYlrTVAYTUG9f8kFM%2FnQL5qPDZw%3D|CDM|1.3|{"originWidth":717,"originHeight":373,"style":"alignCenter","width":655,"height":341}_##]

### ì„±ëŠ¥ì„ ë´…ì‹œë‹¤.Â 

**Machine Translation**Â 

[##_Image|kage@dppGcu/btsOM4d8Xmw/AAAAAAAAAAAAAAAAAAAAACowHO3mblz7Wo1-iI1Hmsj7koe4pZuJDAQPs4JihY85/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=q2ltKQI0%2BhF8IirfD2IfJfixqIg%3D|CDM|1.3|{"originWidth":904,"originHeight":324,"style":"alignCenter"}_##]

**document generation**

[##_Image|kage@cH560F/btsOLFmjPYx/AAAAAAAAAAAAAAAAAAAAAOP0FZ61m9K6cZ_N3KXJbor5XqMzgrXt2snrP6Ye7Nwv/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=j8JxrmMrQuHJFhW3LlvSN80Bt5U%3D|CDM|1.3|{"originWidth":1068,"originHeight":399,"style":"alignCenter"}_##]

TransformerëŠ” ë³‘ë ¬ì²˜ë¦¬ê°€ ì˜ ë˜ê¸° ë•Œë¬¸ì— pre-trainingìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆê³ , ì‚¬ì‹¤ìƒ NLPì˜ í‘œì¤€ì´ ë˜ì—ˆë‹¤.Â 

### Transformerì˜ í•œê³„Â 

**1\. Training instabilities (Pre vs Post norm)**

trainingì´ ë¶ˆì•ˆì •í•¨! íŠ¹íˆ LayerNormì„ ì–´ë””ì— ë„£ëŠëƒì— ë”°ë¼ í•™ìŠµ ì•ˆì •ì„±ì´ ë‹¬ë¼ì§

-   **Pre-norm**: LayerNormì„ **Residual ì „ì—** ì ìš© (ìš”ì¦˜ ì´ ë°©ì‹ì´ ë” ì•ˆì •ì ì„)
-   **Post-norm**: Residual **í›„ì—** ì ìš© (ì´ˆê¸° Transformer ë…¼ë¬¸ ë°©ì‹)

[##_Image|kage@wZUBY/btsOMjJFANk/AAAAAAAAAAAAAAAAAAAAACaRrxSwZ-2TCwVSz2LENOjyyg5tRODrIAzZ62b7-54O/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=rZUnNPBpCdLtNZdbLYrXb%2BJjxOY%3D|CDM|1.3|{"originWidth":986,"originHeight":543,"style":"alignCenter","width":566,"height":312}_##]

**2\. Quadratic comput in self-attention**Â 

self-attention ì€ ê³„ì‚°ëŸ‰ì´ sequence ê¸¸ì´ì˜ ì œê³±ì— ë¹„ë¡€í•¨Â 

ì™œëƒí•˜ë©´

ëª¨ë“  tokenìŒë§ˆë‹¤ attentionì„ ê¼ì‚°í•˜ê¸° ë•Œë¬¸ì— O(n^2) ì„

ê·¼ë° ì™œ ì¨??? Transformerê°€ ì»¤ì§ˆ ìˆ˜ë¡ ì „ì²´ ê³„ì‚°ì—ì„œ self-attentionì´ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì´ ì ì  ì¤„ì–´ë“ ë‹¤. ì¦‰ self-attention ê³„ì‚°ì€ ì—¬ì „íˆ ëŠë¦¬ì§€ë§Œ, ì „ì²´ ëª¨ë¸ ê³„ì‚° ì¤‘ ì¼ë¶€ì¼ ë¿Â 

\- ê²Œì‚°ì„ ì¤„ì´ë ¤ê³  ë§Œë“  **ì €ë ´í•œ(íš¨ìœ¨ì ì¸) Self-Attention ë°©ë²•ë“¤**ì€

â†’ **ëª¨ë¸ì´ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ë³„ë¡œ ì•ˆ ì¢‹ìŒ.**  
â†’ ê·¸ë˜ì„œ ì‹¤ì œ ëŒ€í˜• ëª¨ë¸ì—ëŠ” ì˜ ì•ˆ ì“°ì¸ë‹¤.

**\- Systems optimizations work well (FlashAttention â€“ Jun 2022)**

â†’ ê·¸ë˜ì„œ **ì•Œê³ ë¦¬ì¦˜ì„ ë°”ê¾¸ëŠ” ëŒ€ì‹ **,  
â†’ **ê¸°ì¡´ Attentionì„ ë” ë¹ ë¥´ê²Œ êµ¬í˜„í•œ ìµœì í™” ê¸°ìˆ **ì´ ë§ì´ ì“°ì„.

### ì .ì‹œ.ë§Œ Encoder-Decoder êµ¬ì¡°ì—ì„œ Decoderì˜ ì…ë ¥ì´ Encoderì˜ outputì´ ì•„ë‹ˆì—ˆì–´???

> âœ… **Transformerì˜ DecoderëŠ” Encoderì˜ ì¶œë ¥ì„ "ì§ì ‘ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ê²Œ ì•„ë‹ˆë¼", Cross-Attentionì„ í†µí•´ "ì°¸ì¡°"ë§Œ í•´.**

ì¦‰:  
**Decoderì˜ ì…ë ¥ì€ "ìƒì„± ì¤‘ì¸ ë¬¸ì¥"ì´ê³ **,  
**Encoderì˜ ì¶œë ¥ì€ "ì°¸ì¡° ëŒ€ìƒ"ì¼ ë¿ì´ì•¼.**

---

## ğŸ” ì •ë¦¬í•´ë³´ì:

### Transformer Encoder-Decoder êµ¬ì¡°ì—ì„œì˜ ì—­í• 

ì»´í¬ë„ŒíŠ¸ì…ë ¥ì²˜ë¦¬ì¶œë ¥

| **Encoder** | ì „ì²´ source ë¬¸ì¥ (ex: ì˜ì–´) | Full self-attention | ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ ë²¡í„° |
| --- | --- | --- | --- |
| **Decoder** | ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ target ë¬¸ì¥ (ex: ë¶ˆì–´ ì¼ë¶€) | Masked self-attention + Cross-attention | ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ |

---

## ğŸ“Œ ì¤‘ìš”í•œ êµ¬ë¶„

### âŒ Decoderì˜ ì…ë ¥ì´ "Encoder ì¶œë ¥"ì´ë¼ëŠ” ì˜¤í•´

-   âŒ DecoderëŠ” Encoderì˜ ì¶œë ¥ ë²¡í„°ë“¤ì„ inputìœ¼ë¡œ "ë„£ëŠ”" ê²Œ ì•„ë‹˜
-   âŒ DecoderëŠ” Encoderì˜ ì¶œë ¥ì„ ë¬¸ì¥ì²˜ëŸ¼ "feeding"í•˜ì§€ ì•ŠìŒ

---

### âœ… ì‹¤ì œ êµ¬ì¡°ëŠ” ì´ë ‡ê²Œ ìƒê²¼ì–´:

#### Decoder ë‚´ë¶€ì—ëŠ” **ë‘ ê°œì˜ Attention Layer**ê°€ ìˆìŒ:

1.  **Masked Self-Attention**  
    â†’ ì§€ê¸ˆê¹Œì§€ ìƒì„±í•œ ë‹¨ì–´ë“¤ë¼ë¦¬ë§Œ ì°¸ê³   
    â†’ ë¯¸ë˜ ë‹¨ì–´ëŠ” ê°€ë ¤ì ¸ ìˆìŒ
2.  **Cross-Attention**  
    â†’ Encoderì˜ ì¶œë ¥ ì „ì²´ë¥¼ ì³ë‹¤ë³´ë©´ì„œ í•„ìš”í•œ ì •ë³´ ì°¸ì¡°  
    â†’ ì¦‰, **Decoderì˜ Queryê°€ Encoderì˜ Key/Valueë¥¼ "attend"í•˜ëŠ” êµ¬ì¡°**

---

## ğŸ¯ ë‹¤ì‹œ ë§í•´ ìš”ì•½í•˜ë©´

> **DecoderëŠ” Encoderì˜ ì¶œë ¥ì„ inputì²˜ëŸ¼ ë°›ëŠ” ê²Œ ì•„ë‹ˆë¼, cross-attentionì˜ ëŒ€ìƒ(memory bank)ìœ¼ë¡œ í™œìš©í•œë‹¤.**  
> Decoderì˜ **ì‹¤ì œ ì…ë ¥ì€ ì§€ê¸ˆê¹Œì§€ ìƒì„±í•œ target tokenë“¤**ì´ì•¼.

---

## ğŸ’¡ ì‹œê°ì ìœ¼ë¡œ ê·¸ë ¤ë³´ë©´:

css

ë³µì‚¬í¸ì§‘

\[ì…ë ¥ ë¬¸ì¥ (source)\] â†’ \[Encoder\] â†’ context representations â†‘ \[Decoder â† ì´ì „ ë‹¨ì–´ë“¤\] â†’ Masked self-attention â†“ Cross-attention to Encoder output â†“ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ (one token at a time)