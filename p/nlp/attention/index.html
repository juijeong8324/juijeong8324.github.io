<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="본 글은 cs224n를 정리하였습니다."><title>Attention</title><link rel=canonical href=https://juijeong8324.github.io/p/nlp/attention/><link rel=stylesheet href=/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="Attention"><meta property='og:description' content="본 글은 cs224n를 정리하였습니다."><meta property='og:url' content='https://juijeong8324.github.io/p/nlp/attention/'><meta property='og:site_name' content="DDubi's World"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='NLP'><meta property='article:tag' content='Deep Learning'><meta property='article:published_time' content='2025-06-21T20:02:00+09:00'><meta property='article:modified_time' content='2025-06-21T20:02:00+09:00'><meta name=twitter:title content="Attention"><meta name=twitter:description content="본 글은 cs224n를 정리하였습니다."><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_e94bd0a766f2d89e.jpeg width=300 height=235 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>DDubi's World</a></h1><h2 class=site-description>Hello World!</h2></div></header><ol class=menu-social><li><a href=https://github.com/juijeong8324 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>다크 모드</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#core-idea>Core Idea</a></li><li><a href=#basic-idea>Basic Idea</a></li><li><a href=#attention>Attention</a></li><li><a href=#seq2seq-with-attention의-작동-방식>seq2seq with attention의 작동 방식</a></li><li><a href=#in-equations--수식으로>In equations : 수식으로! </a></li><li><a href=#정리>정리..! </a></li><li><a href=#attention-is-awesome>Attention is awesome!</a></li><li><a href=#attention-is-awesome-deep-learning-technique>Attention is awesome Deep Learning technique</a><ol><li><a href=#more-general-definition-of-attention-><strong>More general definition of attention :</strong></a></li></ol></li><li><a href=#정리-직관><strong>정리: 직관</strong></a></li><li><a href=#추가-궁금증-왜-queryst랑-keysinput-hidden-state를-dot-product하는게-정보를-결합하는게-되는거야>추가 궁금증&mldr; 왜 Query(st)랑 Keys(input hidden state)를 dot product하는게 정보를 결합하는게 되는거야? </a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nlp/ style=background-color:#2a9d8f;color:#fff>NLP</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/nlp/attention/>Attention</a></h2><h3 class=article-subtitle>본 글은 cs224n를 정리하였습니다.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>6월 21, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>5 분 정도</time></div></footer></div></header><section class=article-content><p>이전에 RNN의 문제에 대해서 알아보았다.. </p><p>- 병렬 처리 안 됨 </p><p>- 선형 구조에 의존되어 문장을 이해하기 때문에 잘 처리하지 못함! </p><p>위와 같은 문제를 해결하기 위해 나온 solution이 Attention이다. </p><h3 id=core-idea>Core Idea</h3><p>decoder의 각 step에서 <strong>encoder와의 직접 연결을 이용해서,</strong> <strong>source sequence의 특정 part에 집중한다.</strong></p><p>전통적인 RNN은 인코더의 마지막 hidden state 하나만 보고 다음 단어를 예측했다! (정보 손실이 크다!! )</p><h3 id=basic-idea>Basic Idea</h3><p>attention이 없을 때 다음의 아이디어를 떠올렸다&mldr;</p><p>그러면&mldr; hidden state 정보 하나에 이전의 정보가 모두 잘 요약했다고 보장할 수 없으니까&mldr;? 그걸 다 요약하는 거 어때??</p><p>즉,</p><p>Encoder로부터 정보를 전달하는 가장 기본적인 방법은&mldr; encoder의 hidden state들을 평균(average) 내는 것이다!! </p><p>즉, decoder에 Encoder의 context vector를 전달하는 것! </p><p>[##<em>Image|kage@DzINe/btsOL8VNoTS/AAAAAAAAAAAAAAAAAAAAADSpoVkTOAsA7wF9MjZMkmiyPmzuPpUAJixANG3RSohv/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=NXR3m48WTG8XNRTJBf0gAnQSJ3Y%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:513,&ldquo;originHeight&rdquo;:331,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>문장의 encoding을 계산할 떄 모든 hidden state들의 element-wise max 또는 mean을 취하는 것! </p><h3 id=attention>Attention</h3><p>즉 weighted average를 통해서&mldr; weight가 높은 것을 비중으로 학습하는거지!!! </p><p>[##<em>Image|kage@bIAhbq/btsOL3ty6o4/AAAAAAAAAAAAAAAAAAAAACs-ThR3rgXTYPTssuVKhnURBuKVs2n-EG99t9BcnWl</em>/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=Nw566sYGRUABPHE7sXEyNVrjP2U%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:446,&ldquo;originHeight&rdquo;:299,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}_##]</p><p>Attention에는 <strong>Query</strong>, <strong>Keys</strong>, <strong>Values</strong>로 구성되어 있다. </p><p><strong>Query</strong>는 모든 <strong>Keys</strong>들과 부드럽게 매칭되어 0과 1 사이의 weight를 만든다. </p><p>그 weight를 <strong>Key</strong>에 해당하는 <strong>Value</strong>를 곱하고 모두 더한다. </p><p>[##<em>Image|kage@r2Fiy/btsOLFT6U3Y/AAAAAAAAAAAAAAAAAAAAADVSA91a0Adca3w9eUKGdt8oU8SvCWTfee6_W5xrSTp0/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=M1U58ZHRsAVKoxonVNAMmI9N%2ByI%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:344,&ldquo;originHeight&rdquo;:288,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>Lookup table은 key들을 value에 Mapping한 table이다. </p><p><strong>Query</strong>가 특정 <strong>Key</strong>와 일치하면, 그 <strong>Key</strong>에 해당하는 <strong>Value</strong>를 반환한다.  </p><h3 id=seq2seq-with-attention의-작동-방식>seq2seq with attention의 작동 방식</h3><p>[##<em>Image|kage@l4yAK/btsOLEgvH74/AAAAAAAAAAAAAAAAAAAAAPdoukWGC6LQxaiMEPsCv4MI-r_AcVKtzSjoi0XZLDGq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=HKYtM0yCNoBE4S66vj2GEuJPpEE%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:746,&ldquo;originHeight&rdquo;:407,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>빨간색은 Encoder, 초록색은 Decoder 부분이라고 하자! </p><p>1. 각 input의 hidden state에 대해서 현재 st(decoder의 초기화 vector)를 곱해서 Attention scores를 계산한다. </p><p>[##<em>Image|kage@mOuKB/btsOKqQYw8p/AAAAAAAAAAAAAAAAAAAAAGPQ6q_U9671wVdFeZTwEgEA17_CnmsLWW11L6pPpGxS/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=p0B%2BCxP3mBSdvZy36bJcegFs%2FhA%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:593,&ldquo;originHeight&rdquo;:459,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>2. SoftMax를 이용해서 계산한 scores를 probability distribution(이것이 진짜 가중치!)으로 바꿔준다. </p><p>Decoder timestep에서는 우리는 제일 먼저 encdoer hidden state 를 먼저 focusing 한다는 것을 알 수 있다. </p><p>[##<em>Image|kage@bITsJ5/btsOMGRY8EJ/AAAAAAAAAAAAAAAAAAAAAL7KQOnnV7YuFMhrG-J9nAEXHgTGVkMlCT4YyZ4wKEsI/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=mB%2BI%2Fdz6tTmEBS3MlJA%2BryCYPrU%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:742,&ldquo;originHeight&rdquo;:479,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:496,&ldquo;height&rdquo;:320}</em>##]</p><p>3. 이 attention distribution(가중치)을 이용해서 encoder hidden state의 가중합을 계산한다. </p><p>계산된 attention output은 대부분 high attention을 가진 hidden state의 정보를 포함하게 된다. </p><p>[##<em>Image|kage@8IDHU/btsOKQomlFc/AAAAAAAAAAAAAAAAAAAAAFTLQ5a8Hx_aJPumAAmX_MDdOpwmrwrXYQDGH8c5z0FW/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=JUbcSl3rBUdudzevAyWIS3496fU%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:736,&ldquo;originHeight&rdquo;:457,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:581,&ldquo;height&rdquo;:361}</em>##]</p><p>6. attention output (context output)과 decoder hidden state를 연결해서 이를 기반으로 y1을 계산한다. </p><p>[##<em>Image|kage@beZtrw/btsOMPg4qZg/AAAAAAAAAAAAAAAAAAAAAOh0qj7u_Q6kTeCNhG8flJLw-JIeY8Duad0puVbSMqg7/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=k8xTX3QjOZJWZkTfDEckIaq8uXQ%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:748,&ldquo;originHeight&rdquo;:467,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:529,&ldquo;height&rdquo;:330}</em>##]</p><p>어떤 경우에는 이전 step의 attention output도 decoder input에 같이 넣는다. </p><h3 id=in-equations--수식으로>In equations : 수식으로! </h3><p>1. Encoder의 hidden states가 주어지고, timestep t에 대해서 decoder hidden state st가 있다고 하자. </p><p>[##<em>Image|kage@qmnw5/btsOLheU80P/AAAAAAAAAAAAAAAAAAAAABqmL3BeKs1-WwcucDIqZNZNwWTw4RZlWTUCuCAe2i6x/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=YCjEosVPZzeYKkXIP0SesBtVBLk%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:149,&ldquo;originHeight&rdquo;:26,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##][##<em>Image|kage@vdWwJ/btsOMasxrYh/AAAAAAAAAAAAAAAAAAAAAD5-9-XL4cOrEGwkLVzDy1XnV4vVSkZ917stNgHgXpTl/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=sTgDFYKKxqQtemRVdUG5sJJx4CA%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:72,&ldquo;originHeight&rdquo;:27,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>2. 이 step에 대해서 attention scores(단시 유사도 점수!) 를 계산하자! </p><p>[##<em>Image|kage@bqTZ13/btsOKIcOqUM/AAAAAAAAAAAAAAAAAAAAAOzDKAMvg_w2UBQGSoDcRJ4CtMkT_RwxyMy-PXABjB1t/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=YjzML%2FZ8lXjTYNfgV49kEqPRS5w%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:319,&ldquo;originHeight&rdquo;:54,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>즉 현재 st(<strong>Query</strong>)와 입력의 hidden state(</p><p><strong>Values</strong></p><p>)를 곱하면 된다. </p><p>3. 이 step의 attention distribution(<strong>attention Weight</strong>)을 계산한다. softmax 함수로! (확률 분포를 계산해주고, 합은 1)</p><p>즉, 각 st가 encoder hidden state에 대해서 얼마나 집중할지 확률처럼 해석 가능!! </p><p>[##<em>Image|kage@bEvDUD/btsOKAsxBCz/AAAAAAAAAAAAAAAAAAAAAN3SAvBNcgPrYThoCliDXHKIj6l-4fUpJYJYpwck8bDS/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=I3MdRz9DP3w8zkXggsLSnnuWm2c%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:264,&ldquo;originHeight&rdquo;:52,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>4. 이 attention distribution(<strong>attention</strong> <strong>Weight</strong>)을 encoder hidden state(<strong>Values</strong>)와 weighted sum하여 attention output를 얻는다. </p><p>[##<em>Image|kage@biOpXj/btsOMPaiSA6/AAAAAAAAAAAAAAAAAAAAAFZ8T5xInYjJeBn7dAg5MluLo2fCkUByTSaICLoT7Jri/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=o15kVaoMIH3jFZ72I8ynHHSJREE%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:192,&ldquo;originHeight&rdquo;:73,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>5. 이 attention output at와 decoder hidden state st를 이어 붙이고 decoder로 넘겨준다!! </p><h3 id=정리>정리..! </h3><div class=table-wrapper><table><thead><tr><th><strong>Query qtq_t</strong></th><th>디코더의 현재 hidden state (t 시점)</th><th>“지금 어떤 정보가 필요하지?”</th></tr></thead><tbody><tr><td><strong>Keys kik_i</strong></td><td>인코더의 각 hidden state</td><td>입력 단어들의 &ldquo;정체성&rdquo;</td></tr><tr><td><strong>Values viv_i</strong></td><td>보통 Keys와 같고, 인코더의 hidden state 자체</td><td>입력 단어들의 실제 정보</td></tr><tr><td><strong>Attention distribution</strong></td><td>Query와 각 Key 간의 유사도 → softmax로 만든 가중치들</td><td>α1,α2,…,αT\alpha_1, \alpha_2, \dots, \alpha_T, 총합 = 1</td></tr><tr><td><strong>Attention output (context vector)</strong></td><td>∑iαi⋅vi\sum_i \alpha_i \cdot v_i</td><td>집중한 값들의 가중 평균</td></tr></tbody></table></div><h3 id=attention-is-awesome>Attention is awesome!</h3><p>Attention은 parallelizable하고 </p><p>bottlenck issues를 해결할 수 있다. </p><p>즉, 각 단어의 representation(==vector)을 Query로 사용해서 Values에서 정보를 가져오고 결합한다. </p><p>== 각 단어(Query)는 모든 단어를 탐색하여(key) 자기에게 필요한 정보만 골라낸다! </p><p>우리가 지금까지 본 것은 <strong>Cross-Attention</strong>(Decoder가 Encoder를 쳐다보는 것) </p><p>이제는 한 문장 안에서 단어들끼리 서로 주목하는 <strong>Self-Attention</strong> (단어들끼리 서로를 동시에 쳐다봄)을 보자!! </p><p>Self-attention은 특히 <strong>순차적으로 처리해야 하는 연산의 개수</strong>가 sequence 길이에 따라 늘어나지 않는다는 장점이 있다! 즉 GPU에 친화적이다!!</p><p>모든 word가 한 layer에서 서로 상호작용 하기 때문에 최대 interaction distance는 O(1)이다. </p><p>[##<em>Image|kage@bJvhuO/btsOLqQdNqO/AAAAAAAAAAAAAAAAAAAAAPw0Kk8Tq9MJ_Y4-LHuDkA61C7WrHa04922uXhjVYdto/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=dryUYJGjATLAKGTSRH4XsjvRICw%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:460,&ldquo;originHeight&rdquo;:209,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><h3 id=attention-is-awesome-deep-learning-technique>Attention is awesome Deep Learning technique</h3><p>우리는&mldr; seq2seq 모델을 Machine Translation 하기 위해 개선한 방법인 attention을 보았습니다..</p><p>하지만!! 다른 많은 architextures, 그리고 많은 task에서 사용된다다!! </p><h4 id=more-general-definition-of-attention-><strong>More general definition of attention :</strong></h4><p>• Given a set of vector values, and a vector query, attention a weighted sum of the values, dependent on the query.</p><p>어떤 경우에는 query 가 value에 attend한다. 라고 말한다. </p><p>지금까지 우리가 본 seq2seq + attention 구조에서는 Decoder의 각 hidden state (query)가 Encoder의 모든 hidden state (value)에 attention한다. (즉, 어떤 정보가 필요한지 attention을 통해 결정!)</p><h3 id=정리-직관><strong>정리: 직관</strong></h3><p>가중합 : query가 집중해야 할 값을 결정한 values들이 포함된 정보의 선택적 요약, </p><p>Attention은 아무리 많은 representation이 있어도 하나의 고정된 fixed-size representation(vector)을 뽑아낸다. </p><p>그 vector는 query가 어디에 집중했는지에 따라 달라진다. </p><p>Attention은 거의 모든 Deep Learning model에서 가장 강력하고 유연하고 일반적인 pointer이자 memory manipulation이다. </p><p>그래서 이 아이디어가 적용된게 NMT여! </p><h3 id=추가-궁금증-왜-queryst랑-keysinput-hidden-state를-dot-product하는게-정보를-결합하는게-되는거야>추가 궁금증&mldr; 왜 Query(st)랑 Keys(input hidden state)를 dot product하는게 정보를 결합하는게 되는거야? </h3><p>dot product는 결국 두 입력이 얼마나 관련 있는지 (유사도)를 측정하는 것! 그래서 Query(st)랑 Key(입력)얼마나 관련이 있는지 측정하고 value(=정보)를 얼마나 가져올지 결정하는게 정보 결합!!!!</p><p>근데 위의 모델에서는<br><strong>Key = Value = Encoder Hidden State</strong>로 그냥 써버린 것!! </p><p>Transformer에서는 Key, Value는 hidden state에 다른 가중치 marix를 곱한다.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/nlp/>NLP</a>
<a href=/tags/deep-learning/>Deep Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/nlp/pretraining/><div class=article-details><h2 class=article-title>Pretraining</h2></div></a></article><article><a href=/p/nlp/transformer-encoder/><div class=article-details><h2 class=article-title>Transformer - Encoder</h2></div></a></article><article><a href=/p/nlp/transformer-decoder/><div class=article-details><h2 class=article-title>Transformer - Decoder</h2></div></a></article><article><a href=/p/nlp/transformer1-self-attention/><div class=article-details><h2 class=article-title>Transformer(1) - Self-attention</h2></div></a></article><article><a href=/p/nlp/machine-translation-mt%EC%9D%98-%EB%B0%9C%EC%A0%84/><div class=article-details><h2 class=article-title>Machine Translation (MT)의 발전</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 DDubi's World</section><section class=powerby><a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>로 만듦<br><a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>의 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 테마 사용 중</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>