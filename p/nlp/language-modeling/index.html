<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="본 글은 cs224n를 정리하였습니다."><title>Language Modeling</title><link rel=canonical href=https://juijeong8324.github.io/p/nlp/language-modeling/><link rel=stylesheet href=/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="Language Modeling"><meta property='og:description' content="본 글은 cs224n를 정리하였습니다."><meta property='og:url' content='https://juijeong8324.github.io/p/nlp/language-modeling/'><meta property='og:site_name' content="DDubi's World"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='NLP'><meta property='article:tag' content='Deep Learning'><meta property='article:published_time' content='2025-06-21T14:10:00+09:00'><meta property='article:modified_time' content='2025-06-21T14:10:00+09:00'><meta name=twitter:title content="Language Modeling"><meta name=twitter:description content="본 글은 cs224n를 정리하였습니다."><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_e94bd0a766f2d89e.jpeg width=300 height=235 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>DDubi's World</a></h1><h2 class=site-description>Hello World!</h2></div></header><ol class=menu-social><li><a href=https://github.com/juijeong8324 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>다크 모드</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#language-modeling의-정의>Language Modeling의 정의  </a></li><li><a href=#language-modeling을-왜-신경써야-할까>Language Modeling을 왜 신경써야 할까? </a></li><li><a href=#next-word-prediction>Next word prediction </a></li><li><a href=#예전에는-어떻게-language-model을-학습시켰을까>예전에는 어떻게 Language Model을 학습시켰을까?</a><ol><li><a href=#문제-1>문제 1</a></li><li><a href=#문제-2>문제 2</a></li><li><a href=#문제-3>문제 3</a></li></ol></li><li><a href=#neural-language-model>Neural Language Model? </a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nlp/ style=background-color:#2a9d8f;color:#fff>NLP</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/nlp/language-modeling/>Language Modeling</a></h2><h3 class=article-subtitle>본 글은 cs224n를 정리하였습니다.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>6월 21, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 분 정도</time></div></footer></div></header><section class=article-content><h2 id=language-modeling의-정의>Language Modeling의 정의  </h2><p><em>다음에 올 단어를 예측하는 task를 수행한다</em></p><p>EX) the students opened their ____ -> (books, laptops, exams, minds)</p><p><em>주어진 단어들의 sequence(== 문장)</em></p><p><em>x1, x2, ,,,, xt</em></p><p><em>다음 단어 xt+1의 probability distribution를 계산하는 것</em> </p><p><em>x+1은 vocabulary V= {w1, &mldr; , wV}의 아무 단어를 의미한다.</em> </p><p><em>위와 같은 작업을 수행하는 것이 Language Model이다.</em> </p><p>쉽게 생각해서 단더 조각의 probabiltiy를 할당해주는 system이라고 생각하자! </p><p>[##<em>Image|kage@bEu4UG/btsOMbx4VHd/AAAAAAAAAAAAAAAAAAAAALAuedCmtWhEmgVvyRDKLlYNoK0C_VgaLV7x_pUgvez8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=c1L%2Ft7T%2BP3fKyowIDA7ity6N4xU%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:628,&ldquo;originHeight&rdquo;:179,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>그러니까 즉, 어떤 문장이 생성되기 위해서는 첫 번째 단어가 올 확률 x 첫 번째 단어가 있을 때 2번째 단어가 올 확률 x &mldr;</p><p>쉽게 생각하면 input은 그동안 생성한 글의 확률, output은 그 글의 다음 단어의 확률 </p><h2 id=language-modeling을-왜-신경써야-할까>Language Modeling을 왜 신경써야 할까? </h2><p>Language Modeling는 <strong>benchmark task</strong>이다!</p><p>이는 우리의 progress가 잘 예측하는지 측정하는데 도움이 된다. </p><p>또한 많은 NLP(Natural Language Process)의 subcomponent라고 보면 된다.</p><p>왜냐하면 <strong>text 생성 이나 text의 확률을 예측</strong>하는 일에서 매우 중요하다ㅏ. </p><p>•  Predictive typing<br>• Speech recognition<br>• Handwriting recognition<br>• Spelling/grammar correction<br>• Authorship identification<br>• Machine translation<br>• Summarization<br>• Dialogue</p><p>결론</p><p>NLP의 모든 기술은 LM을 기반으로 다시 설계된 것.</p><p>옛날에는 NLP 각 task마다 model이 따로 존재(감정 분석: SVM, 번역: RNN, 요약 Seq2Seq&mldr;)</p><p>요즘은 대규모 언어 모델(GPT, Bert) 하나로 모든 걸 해결! </p><h2 id=next-word-prediction>Next word prediction </h2><p>[##<em>Image|kage@B880X/btsOK56w4m9/AAAAAAAAAAAAAAAAAAAAAGi7MTKKVksaHyg8oGF0rKmbkx07-XeOS6WYhS79W1rJ/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=ZvaUY0HkP726Ec1gIbbi2I5wAXM%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:876,&ldquo;originHeight&rdquo;:272,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:699,&ldquo;height&rdquo;:217}</em>##]</p><p>이런식으로 문장 안의 blank를 두어서 예측을 하도록 학습시켰다면&mldr; </p><p>[##<em>Image|kage@brGrYk/btsOMbrhW2U/AAAAAAAAAAAAAAAAAAAAAJVZ7cxHL1j8l05ObJ55ytt-g0YBZVYRgco6NaEaiMlP/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=I736lUxcPUJPfuxX3GzgIH38w3U%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:768,&ldquo;originHeight&rdquo;:364,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:684,&ldquo;height&rdquo;:324,&ldquo;caption&rdquo;:&ldquo;Context와 example 하나를 보여주고 질문에 답을 하게 한다.&rdquo;}</em>##]</p><p>요즘은 이렇게 GPT 같은 모델들이 <strong>in-context learning</strong> 으로 작업을 수행하게 한다. </p><p>즉, 몇 개의 example을 주어지면 그 작업을 수행하게 한다. </p><p>fine-tuning(훈련)하지 않고 propmt 상에서 example 몇 개(한 개 혹은 그 이상)만 보여주어서 새로운 작업을 수행하게 한다. </p><h2 id=예전에는-어떻게-language-model을-학습시켰을까>예전에는 어떻게 Language Model을 학습시켰을까?</h2><p><strong>n-gram Language Model!</strong></p><p><strong>Markov assumption</strong></p><p>현재 상태는 오직 직전 상태에 의존한다. 과거의 모든 정보는 직전 상태(n이면 n-1개까지의 과거)에 요약되어 있다! </p><p>자연어 처리에서는&mldr;   </p><p>[##<em>Image|kage@buCTf6/btsOLCCYRPY/AAAAAAAAAAAAAAAAAAAAAEJfukJ_v1O9xuY3QzTuz7i_7f-TsttY1Bv7BWikBI8k/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=xhloXgei4u44u82gqR%2B99W%2FZtIk%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:762,&ldquo;originHeight&rdquo;:114,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>xt+1(현재 단어)은 이전의 n-1 개의 단어에(과거의 정보) 의존한다.</p><p>[##<em>Image|kage@cpa9dj/btsOKBkD6Gh/AAAAAAAAAAAAAAAAAAAAAE1v60qR30vxs8aj35g8PCBZX72lX-Cy40UCZ6SEp8ps/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=qIY0LUvEpKo1VTag8E7SzXTM%2Feo%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:614,&ldquo;originHeight&rdquo;:187,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>만약 4-gram Language Model이라고 하면 3개의 단어를 확인 </p><p>이를 직접 statistical approximation 하게 count 한다. </p><p>[##<em>Image|kage@MjTJo/btsOLvqmrvI/AAAAAAAAAAAAAAAAAAAAAPGTzzs7bDTPGiHL-aOJuJUES5godmZjlxljy8xrcZvy/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=9ArLFBmwn2JsNsaf4ukucZ4IMwA%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:730,&ldquo;originHeight&rdquo;:202,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>즉 corpus에서 3개의 단어가 1000개 나왔을 때 다음 단어를 count해서 학습 </p><p>하지만 proctor 라는 context(문맥)을 아예 무시해도 될까??</p><h3 id=문제-1>문제 1</h3><p>만약 data에 &ldquo;students opened their w&rdquo; 이 아예 나타나지 않으면? </p><p>-> 작은 입실론을 추가해준다. </p><h3 id=문제-2>문제 2</h3><p>&ldquo;students opened their&rdquo; 가 아예 없으면?</p><p>-> backoff로 &ldquo;opened their&rdquo; 로 대신해라!  </p><h3 id=문제-3>문제 3</h3><p>그리고 그 모든 확률을 다 count 해야 하는데..? </p><p>-> storage 문제 발생 </p><p>사실 n=5이상이면 문제가 발생해서&mldr; 잘 안 쓴다. </p><p>근데&mldr;</p><p>incohrent하다!! 즉 앞뒤가 안 맞는 문장이 생성된다. </p><p>적어도 3개 이상의 단어를 고려해야 하는데&mldr; n이 커지면 위와 같은 문제가 발생하고&mldr; 어뜩하지 </p><h2 id=neural-language-model>Neural Language Model? </h2><p>즉 확률을 모델링하는 것이 아니라&mldr; Deep Learning(신경망)을 이용해서 확률을 모델링하자! </p><p>[##<em>Image|kage@UZADO/btsOKDicr3a/AAAAAAAAAAAAAAAAAAAAAKs73WHd8Jz4kU62qvLWkSTEREbXI-zd3iVBIIyhRW8t/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=bLSWe3euV220bNV8vjgc99woGa4%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:721,&ldquo;originHeight&rdquo;:495,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>즉 위와 같이 vector를 만들어서 embedding을 해주고 hidden layer를 통해 계산. </p><p>따라서 n-gram LM의 문제를 해결한다! </p><p>관찰되어야 할 n-gram을 모두 저장할 필요가 없고 </p><p>정보가 존재하는지 아닌지도 문제 (sparsity problem) 도 없어진다!</p><p>그러나&mldr; 다음과 같은 한계가 존재하는데&mldr;.</p><ul><li>Fixed window도 너무 작다. </li><li>window size를 키우면 W(가중치)도 더 커진다!!!</li><li>즉, 매우 긴 sequnce를 이해할 수 없다!! (문장 앞부분의 개념이 뒷부분 해석에 중요할 수도 있으니!)</li><li>x1과 x2는 곱해지는 가중치가 다르다! (즉, 입력 위치에 따라 처리 방식이 달라진다) 이런 경우 입력 위치가 다르면 같은 단어라도 다르게 처리될 수도..(즉 dog라는 의미는 앞이든 뒤이든 같아야 함!!) </li></ul><p>어떤 길이의 input이든 처리할 수 있는 neural architecture가 필요하다!!! </p><p>이제&mldr; RNN 을 배울 때다!!</p></section><footer class=article-footer><section class=article-tags><a href=/tags/nlp/>NLP</a>
<a href=/tags/deep-learning/>Deep Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/nlp/pretraining/><div class=article-details><h2 class=article-title>Pretraining</h2></div></a></article><article><a href=/p/nlp/transformer-encoder/><div class=article-details><h2 class=article-title>Transformer - Encoder</h2></div></a></article><article><a href=/p/nlp/transformer-decoder/><div class=article-details><h2 class=article-title>Transformer - Decoder</h2></div></a></article><article><a href=/p/nlp/transformer1-self-attention/><div class=article-details><h2 class=article-title>Transformer(1) - Self-attention</h2></div></a></article><article><a href=/p/nlp/attention/><div class=article-details><h2 class=article-title>Attention</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 DDubi's World</section><section class=powerby><a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>로 만듦<br><a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>의 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 테마 사용 중</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>