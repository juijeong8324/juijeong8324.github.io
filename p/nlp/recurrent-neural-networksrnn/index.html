<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="본 글은 cs224n를 정리하였습니다."><title>Recurrent Neural Networks(RNN)</title><link rel=canonical href=https://juijeong8324.github.io/p/nlp/recurrent-neural-networksrnn/><link rel=stylesheet href=/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="Recurrent Neural Networks(RNN)"><meta property='og:description' content="본 글은 cs224n를 정리하였습니다."><meta property='og:url' content='https://juijeong8324.github.io/p/nlp/recurrent-neural-networksrnn/'><meta property='og:site_name' content="DDubi's World"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='NLP'><meta property='article:tag' content='Deep Learning'><meta property='article:published_time' content='2025-06-21T16:53:00+09:00'><meta property='article:modified_time' content='2025-06-21T16:53:00+09:00'><meta name=twitter:title content="Recurrent Neural Networks(RNN)"><meta name=twitter:description content="본 글은 cs224n를 정리하였습니다."><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_e94bd0a766f2d89e.jpeg width=300 height=235 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>DDubi's World</a></h1><h2 class=site-description>Hello World!</h2></div></header><ol class=menu-social><li><a href=https://github.com/juijeong8324 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>다크 모드</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#core-idea>Core Idea</a></li><li><a href=#rnn의-전체-구조>RNN의 전체 구조 </a><ol><li><a href=#장점>장점</a></li><li><a href=#단점>단점 </a></li></ol></li><li><a href=#training>Training </a></li><li><a href=#문제점>문제점</a></li><li><a href=#backpropagation-for-rnns-w-parameter를-훈련-시키기-위함>Backpropagation for RNNs (W parameter를 훈련 시키기 위함!) </a></li><li><a href=#problems-with-rnns-vanishing-and-exploding-gradients>Problems with RNNs: Vanishing and Exploding Gradients</a></li><li><a href=#vanishing-문제를-어떻게-해결해야-할까>Vanishing 문제를 어떻게 해결해야 할까? </a></li><li><a href=#추가-질문>추가 질문</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nlp/ style=background-color:#2a9d8f;color:#fff>NLP</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/nlp/recurrent-neural-networksrnn/>Recurrent Neural Networks(RNN)</a></h2><h3 class=article-subtitle>본 글은 cs224n를 정리하였습니다.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>6월 21, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>5 분 정도</time></div></footer></div></header><section class=article-content><h3 id=core-idea>Core Idea</h3><p>[##<em>Image|kage@cd3kQx/btsOL55YG2J/AAAAAAAAAAAAAAAAAAAAANYIGed86TNJRULXuEeGhsPDL1fSZKTEgFCyVcJT_nnM/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=xtXMYU30gOU9Dw%2FwaYztfGNHw2E%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:620,&ldquo;originHeight&rdquo;:302,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>같은 가중치 W를 반복해서 적용한다!!!!</p><p>각 입력에 대해서 hidden state에 같은 W를 곱한것을 같이 처리하는 것을 볼 수 있다. (이전에는 x1은 w1, x2는 w2랑 곱해져왔음) </p><h3 id=rnn의-전체-구조>RNN의 전체 구조 </h3><p>[##<em>Image|kage@nAlnd/btsOKCKGYh2/AAAAAAAAAAAAAAAAAAAAAI_fpqbVrSvAnP94hHCY_7jcCxy1K1hw80WqHlf7PRzI/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=agopE1K1jweMPlppzIrc%2BVSfsUY%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:761,&ldquo;originHeight&rdquo;:525,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><h4 id=장점>장점</h4><ul><li>어떤 길이의 input이든 처리 가능</li><li>step t에 대한 계산은 많은 뒤의 step의 정보를 활용한다.(이전 정보를 활용한다!) </li><li>input 크기에 따라 Model size(Wh, We)가 증가하지 않는다. </li><li>모든 시간 step t에 같은 W를 적용하기 때문에 위치에 상관없이 공정하게 처리된다. </li></ul><h4 id=단점>단점 </h4><ul><li>Recurrent computation이 너무 느려 -> 병렬 처리가 안 된다!! </li><li>완전 이전 step의 정보를 얻는게 좀 어려움&mldr; -> 정보 소실 문제! (gradient vanishing)</li></ul><h3 id=training>Training </h3><ul><li>Input : 단어들의 sequence</li><li>Ouput : 모든 각 step t의 distribution yt</li><li>Loss : step t에 대해서, 예측 확률 분포 yt와 진짜 다음 단어 yt 간의 cross-entoropy(불일치를 측정하는 함수)를 계산 </li><li>그 Loss를 모두 더해서 평균을 낸다.  </li></ul><p><strong>* cross-entropy?</strong></p><p>예측한 확률 분포yt와 실제 정답(label)의 분포 yt 사이의 불일치를 측정, 모델에 확신을 가지고 맞췄는지를 평가하는 Loss 함수! </p><p>따라서 정답을 맞춰도 확신이 없으면 Loss가 커진다&mldr; </p><p>[##<em>Image|kage@bu6ZvL/btsOL8OXDOx/AAAAAAAAAAAAAAAAAAAAANYGFDr1VnpHth2STLiZISwfe_wqt8My-valyxnwKa7Y/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=OBFojuUdiS4Ro%2FJuzjm6fOXkAGA%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:616,&ldquo;originHeight&rdquo;:255,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##][##<em>Image|kage@qz5G4/btsOK40UBYS/AAAAAAAAAAAAAAAAAAAAAPgHM10-ONI6fduzzG8qC55ZS0yeKIB5r3a_e-eiFBq3/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=Ln447zxoSPmQ9%2B6wXoRP9D%2Fvt%2BM%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:839,&ldquo;originHeight&rdquo;:522,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:681,&ldquo;height&rdquo;:424}</em>##]</p><p>1. x1이 들어와서 y1을 예측</p><p>2. 이때 Loss는 실제 true값(y2) 와의 nevative log prob을 계산한다. </p><p>3. time step t마다 계산 후에 </p><p>4. Loss를 계산 후 평균 </p><h3 id=문제점>문제점</h3><p>전체 corpus(문장, document)에 대해서 loss와 grandient를 한번에 계산하는 것은 너무 비쌈.. 메모리 문제.. </p><p>그래서 </p><p>Stochastic Gradient Descent 기억하니??? 이러한 작은 data chunk에 대해서 loss와 gradient를 계산하고 update..</p><p>이걸 적용해보겠다&mldr; </p><p>그래서 한 문장에 대해서 loss를 계산하고(원래는 여러 문장 batch에 대해서 계산했었음) graeidnt랑 weight를 업데이트 해서.. 이제 다른 새로운 한 문장에 대해서 반복한다~  </p><h3 id=backpropagation-for-rnns-w-parameter를-훈련-시키기-위함>Backpropagation for RNNs (W parameter를 훈련 시키기 위함!) </h3><p>[##<em>Image|kage@mg3iq/btsOK3gBiFm/AAAAAAAAAAAAAAAAAAAAAOoIiVeMS4EAFLxLbEQdFCnjBPX6LasLXf1C3cmF95GE/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=pAazc3X5vbWcsh65DO1koqUo68s%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:756,&ldquo;originHeight&rdquo;:419,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:612,&ldquo;height&rdquo;:339}</em>##]</p><p>가중치 matrix Wh에 대해서 Jt의 미분은 무엇인가? </p><p>여러 위치에서 반복적으로 사용된 같은 가중치에 대한 gradient는 각각의 사용 위치에서 나온 gradient들을 모두 더한것! </p><p>즉, 각 timestep에서 loss에 기여한 W에 gradient가 따로 생김! 이 W에 대해 업데이트 하려면 모든 timestep에서 나온 gradient를 한번에 더해야해! </p><p>[##<em>Image|kage@3lKBN/btsOLfnQEAb/AAAAAAAAAAAAAAAAAAAAAPjbKuP46Dg-E-VCUPX_CFF7mHCsyRNM6Q1Is8H4ZAvF/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=u%2BO2hvqDSwJXiSFX%2Bq0GuR%2FeUeU%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:872,&ldquo;originHeight&rdquo;:459,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>timestep를 따라 거꾸로 역전파하면서(t, t-1, .. 0) 각 시점마다의 gradient를 계산해서 전부 더한다. </p><p>따라서 backpropatgation through time (BPTT) </p><p>그니까 시간을 거꾸로 돌면서&mldr;! 계산한다! </p><p>마치 n-gram LM같음! 반복되는 sampling을 통해서 text를 생성하는~~</p><h3 id=problems-with-rnns-vanishing-and-exploding-gradients>Problems with RNNs: Vanishing and Exploding Gradients</h3><p>그러면 이제 각 timestep t에 대해서 Wh에 대한 J4의 gradient를 계산해야 함! </p><p>이때 시점이 1일 때 J4/Wh</p><p>[##<em>Image|kage@c6Mtp5/btsOMqIBmyj/AAAAAAAAAAAAAAAAAAAAAKBM8pAVxyGZw73TMjsAJ0pmdqhp4Zt-xEYITKJG6djN/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=3BiU%2FM%2BvC7sZD6aur%2BHCTsfdFps%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:595,&ldquo;originHeight&rdquo;:350,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>우리가 4번째 Loss를 계산하고 이것을 첫 번째 W(hidden state)에 대한 가중치를 계산해야 함! </p><p>[##<em>Image|kage@c41Eec/btsOKq4vd6Y/AAAAAAAAAAAAAAAAAAAAAJdDrLhfcBcbPxKpllFz0izw43toU0HB8hVXWQxP56yA/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=e7hJ%2BvM10%2FBDDhCsVZL1ldD2sT0%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:799,&ldquo;originHeight&rdquo;:511,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:458,&ldquo;height&rdquo;:293}</em>##]</p><p>이를 위해서는 chain rule에 따라 위와 같이 계산된다.. </p><p>그런데&mldr; 곱해지는 저 값들이 작으면 어떻게 되니? </p><p><strong>Vanishing gradient problem</strong></p><p>backpropagation이 더 멀리 진행될 수록 gradient signal이 더 작이지고 작아지는 현상</p><p>[##<em>Image|kage@cSSAqq/btsOLwJyfLB/AAAAAAAAAAAAAAAAAAAAANx0D3CcClzEqW80cTkfeIK2NffZKwAtp-HYRQzx45o5/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=MM5pOJ%2Fsdg7lqRrGlRThWq9NhhQ%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:770,&ldquo;originHeight&rdquo;:409,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>그러니까&mldr; ht 함수에는 wh 라는 우리가 계산해야 할 가중치가 있고 또 그 안의 ht-1 함수에는 우리가 계산해야 할 가중치가 있다. chain으로 곱채지는데&mldr; 곱해지는 값이 작은 수면.. 작은 수 끼리 계속 곱해진다&mldr; 따라서 지수적으로 작아진다는 문제가 발생</p><p>[##<em>Image|kage@UOpMn/btsOLdDCn8y/AAAAAAAAAAAAAAAAAAAAAJ1SE3Q95f-Db_u_s_neu4yQdaqc7_iRYuoKmdU0ES1p/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=pPPO1mCmNROOkzinLjcn9Q5Z7Bs%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:916,&ldquo;originHeight&rdquo;:455,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>그래서 문제는&mldr; backpropagation을 할 때 timestep이 올라갈 수록 점점 그 값이 작아져서 .. 나랑 가까운 시점의 gradient는 크고 나랑 먼 시점의 gradient는 기억하지 못한다는 단점이 있다. 즉, 짧은 기억만 가능하다!!! 긴 문장 처리 못 함!! </p><p>즉 어떤 t 시점의 Loss와 W를 계산하려면 i 시점의 gradient 즉, Loss t / hi를 계산해야 하고 이때 i가 t보다 너무 멀면 아예 값이 작아져서 영향을 못 준다는 거구나!!!</p><p>그래서&mldr; </p><p>긴 문장이 주어지고 빈칸을 주어지는 문제를 맞출 때&mldr;! </p><p>When she tried to print her tickets, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her ________</p><p>여기서 답은 ticket 즉, 맨 앞의 7번째 단어를 보고 맞춰야 하는데.. 너무 멀어서 vanishing 되기 때문에.. 못 맞춤.. </p><p>exploding 문제는 걍.. 아예 다른 값이 나올 수 있다는 뜻!!! NaN</p><p>이 문제는 </p><ul><li>Gradient가 일정 threshold보다 크면 잘라주는 작업을 하게 됨! clipping!!</li></ul><h3 id=vanishing-문제를-어떻게-해결해야-할까>Vanishing 문제를 어떻게 해결해야 할까? </h3><p>1. <strong>기억을 따로 보관하고 더해가는 RNN은 어때?</strong>"</p><p>-> LSTM</p><p>2. <strong>정보가 더 직접적으로 흘러가게 (linear pass-through)</strong> 하는 구조</p><p>-> <strong>Attention</strong>이나 <strong>Residual Connection</strong></p><h3 id=추가-질문>추가 질문</h3><p>Q. 여기서 embedding은 word2vector와 같은 pretrained embedding model을 통해 를 통해서 embedding 되는건가?</p><p>No! 그냥 가중치 W에 의해 vector화 해주는거고 pretraining 단계에서 embedding은 자동으로 함께 학습된다. (즉 end-to-end로 같이 학습) </p><p>Q. RNN은 LM인가?</p><p>아님!! RNN은 모델 아키텍쳐이고 LM은 학습 목적이자 Task!!! </p><p>즉 RNN은 시퀀스를 처리하는 신경망 구조이고 </p><p>LM은 텍스트에서 다음 단어 예측 같은 Task를 수행하는 모델 </p><ul><li>너는 <strong>RNN을 써서 Language Model을 만들 수 있어</strong><br>→ 예: RNN-based Language Model (2015년 이전에 많이 사용)</li><li>하지만 <strong>RNN이 항상 Language Model을 의미하는 건 아니야</strong><br>→ 예: RNN으로 음악 생성, 시계열 예측, 번역 등 <strong>비-LM task</strong>도 가능함</li><li>그리고 <strong>Language Model이 꼭 RNN을 써야 하는 것도 아냐</strong><br>→ GPT 같은 모델은 <strong>Transformer 기반 LM</strong></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/nlp/>NLP</a>
<a href=/tags/deep-learning/>Deep Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/nlp/pretraining/><div class=article-details><h2 class=article-title>Pretraining</h2></div></a></article><article><a href=/p/nlp/transformer-encoder/><div class=article-details><h2 class=article-title>Transformer - Encoder</h2></div></a></article><article><a href=/p/nlp/transformer-decoder/><div class=article-details><h2 class=article-title>Transformer - Decoder</h2></div></a></article><article><a href=/p/nlp/transformer1-self-attention/><div class=article-details><h2 class=article-title>Transformer(1) - Self-attention</h2></div></a></article><article><a href=/p/nlp/attention/><div class=article-details><h2 class=article-title>Attention</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 DDubi's World</section><section class=powerby><a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>로 만듦<br><a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>의 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 테마 사용 중</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>