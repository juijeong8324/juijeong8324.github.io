<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="본 글은 cs224n를 정리하였습니다."><title>Transformer - Encoder</title><link rel=canonical href=https://juijeong8324.github.io/p/nlp/transformer-encoder/><link rel=stylesheet href=/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="Transformer - Encoder"><meta property='og:description' content="본 글은 cs224n를 정리하였습니다."><meta property='og:url' content='https://juijeong8324.github.io/p/nlp/transformer-encoder/'><meta property='og:site_name' content="DDubi's World"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='NLP'><meta property='article:tag' content='Deep Learning'><meta property='article:published_time' content='2025-06-22T15:45:00+09:00'><meta property='article:modified_time' content='2025-06-22T15:45:00+09:00'><meta name=twitter:title content="Transformer - Encoder"><meta name=twitter:description content="본 글은 cs224n를 정리하였습니다."><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_e94bd0a766f2d89e.jpeg width=300 height=235 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>DDubi's World</a></h1><h2 class=site-description>Hello World!</h2></div></header><ol class=menu-social><li><a href=https://github.com/juijeong8324 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>다크 모드</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#the-transformer-encoder-decoder><strong>The transformer Encoder-Decoder</strong></a></li><li><a href=#cross-attention-details><strong>Cross-attention (details)</strong></a></li><li><a href=#cross-attention이-계산되는-과정>Cross-attention이 계산되는 과정 </a></li><li><a href=#성능을-봅시다>성능을 봅시다. </a></li><li><a href=#transformer의-한계>Transformer의 한계 </a></li><li><a href=#잠시만-encoder-decoder-구조에서-decoder의-입력이-encoder의-output이-아니었어>잠.시.만 Encoder-Decoder 구조에서 Decoder의 입력이 Encoder의 output이 아니었어???</a></li></ol></li><li><a href=#-정리해보자>🔍 정리해보자:</a><ol><li><a href=#transformer-encoder-decoder-구조에서의-역할>Transformer Encoder-Decoder 구조에서의 역할</a></li></ol></li><li><a href=#-중요한-구분>📌 중요한 구분</a><ol><li><a href=#-decoder의-입력이-encoder-출력이라는-오해>❌ Decoder의 입력이 &ldquo;Encoder 출력"이라는 오해</a></li><li><a href=#-실제-구조는-이렇게-생겼어>✅ 실제 구조는 이렇게 생겼어:</a><ol><li><a href=#decoder-내부에는-두-개의-attention-layer가-있음>Decoder 내부에는 <strong>두 개의 Attention Layer</strong>가 있음:</a></li></ol></li></ol></li><li><a href=#-다시-말해-요약하면>🎯 다시 말해 요약하면</a></li><li><a href=#-시각적으로-그려보면>💡 시각적으로 그려보면:</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nlp/ style=background-color:#2a9d8f;color:#fff>NLP</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/nlp/transformer-encoder/>Transformer - Encoder</a></h2><h3 class=article-subtitle>본 글은 cs224n를 정리하였습니다.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>6월 22, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>5 분 정도</time></div></footer></div></header><section class=article-content><p>[##<em>Image|kage@ts3IR/btsOMTqkBt1/AAAAAAAAAAAAAAAAAAAAAO5eCa8c7NYTnezb7sEzqKBacKSPresrBDlEMbGfNoLD/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=W679xjbXXwiqbkktqAK99VZUDmE%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:602,&ldquo;originHeight&rdquo;:667,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:425,&ldquo;height&rdquo;:471}</em>##]</p><p>Transformer Decoder는 **language model처럼 단방향(unidirectional) context만을 사용하도록 contraint(제한)**한다. </p><p>-> 즉 과거(왼쪽) 정보만 보고 미래(오른쪽)는 보지 않는다. </p><p>-> 이는 Langue model처럼 다음에 올 단어를 예측하는 task</p><p>그러면 우리는 <strong>bidirectional context를 원하면 양방향 RNN처럼 하면 될까?</strong> </p><p>그럴 때 사용하는 것이!!!!! <strong>Transformer Encoder</strong>이다!</p><p>-> 이는 문장 이해, 분류에 적합한 task </p><p><strong>Decoder와의 유일한 차이점은 self-attention에서 masking을 제거한다는 것..! (즉, 모든 단어를 동시에 본다!)</strong> </p><h3 id=the-transformer-encoder-decoder><strong>The transformer Encoder-Decoder</strong></h3><p>[##<em>Image|kage@EsUBF/btsOMe2P77d/AAAAAAAAAAAAAAAAAAAAAMMZMPLAgERAICX1MeRLDaUvqrufJ3FCTwJ4Acj8HCpg/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=lSVxZmTQSeho1FJ6UDsGGzTTBM0%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:648,&ldquo;originHeight&rdquo;:694,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:381,&ldquo;height&rdquo;:408}</em>##]</p><p>Machine translation을 회기하면,, </p><p>우리는 bidirectional model에서 source sentence를 처리했고, unidirectional model에서 target을 생성했다!  </p><p>seq2seq format의 작업에서는&mldr; 이렇게 Transformer Encoder-Decoder 구조를 사용한다!!!!!</p><ul><li><strong>Encoder</strong><br>일반적인 Transformer Encoder를 그대로 사용</li><li><strong>Decoder</strong><br>Encoder의 출력에 대해 <strong>cross-attention</strong>을 수행할 수 있도록 수정된다. Decoder는 <strong>self-attention</strong>으로 자기 자신만 보는게 아니라, Encoder가 만든 input 문장의 context를 참고해서 번역을 생성 </li></ul><h3 id=cross-attention-details><strong>Cross-attention (details)</strong></h3><p>[##<em>Image|kage@c0gxt1/btsOMku0Kfk/AAAAAAAAAAAAAAAAAAAAABG7eNvJgk9_dlDwwjsrY0PNbkeQG-hXvdGsIMMsGfdq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=Wz7T1TPA5m%2BSrkRUCGrk21%2FpPg4%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:597,&ldquo;originHeight&rdquo;:482,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:382,&ldquo;height&rdquo;:308}</em>##]</p><p><strong>self-attention은 같은 source에서 keys, queries, values가 만들어질 때를 말한다.</strong></p><p><strong>즉, Encoder 내부, Decoder 내부</strong> </p><p>Decoder에서는 self-attention도 있지만, 다른 source를 바라보는 attention도 있다!(저번 시간에 배움) 그게 바로 <strong>cross-attention</strong></p><ul><li><strong>h1, &mldr; , hn</strong>: Transformer <strong>encoder</strong> 로 부터 얻은 <strong>output vector(hidden state)</strong> 𝑥𝑖∈ℝ𝑑</li><li><strong>z1, &mldr; , zn</strong>: Transformer <strong>decoder</strong>로 부터 얻은 <strong>input vector(즉, 이미 생성된 단어들의 hidden state)</strong> 𝑧𝑖∈ℝ𝑑</li></ul><p>여기서 keys와 values는 encoder의 출력 hi에서 계산</p><p>즉 Encoder는 일종의 memory 같은 느낌 </p><p>𝑘𝑖=𝐾ℎ𝑖, 𝑣𝑖=𝑉ℎ𝑖</p><p>queries는 decoder로 현재 입력 zi에서 가져온 것 </p><p>𝑞𝑖=𝑄𝑧𝑖.</p><p>그니까 정리하자면&mldr; query는 decoder의 input 우리가 집중해서 보고 싶은 것들은 encoder의 input이고, key와 value에 해당 </p><p>즉 Decoder가 Encoder의 출력에 대해 attention 을 날리는 것! <strong>참조하며 문장을 생성할 수 있게!!</strong></p><h3 id=cross-attention이-계산되는-과정>Cross-attention이 계산되는 과정 </h3><p>[##<em>Image|kage@leV61/btsOLl2zPW4/AAAAAAAAAAAAAAAAAAAAAIA7cXyO2OQFSby4v8-pcRp2XmxzWFnaRyUMSH9ihgPv/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=V2xFp9KXOuxlWZ8yLjJQZxeZ4EI%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:309,&ldquo;originHeight&rdquo;:88,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>H는 encoder vector의 concatenaton! </p><p>Z는 dcoder vector의 concatenation!</p><p>여기서 T는 encoder의 길이 또는 decoder의 길이 중 하나일 수 있는데, <strong>문맥상 둘 다 T로 표기한 거지 실제로는 다를 수 있어.</strong></p><ul><li>예를 들어: 영어 입력(5단어) → 프랑스어 출력(7단어)이라면<ul><li>Encoder: T₁ = 5, Decoder: T₂ = 7</li></ul></li></ul><p>[##<em>Image|kage@cgCHrb/btsOMFyVJLj/AAAAAAAAAAAAAAAAAAAAAAfCTekS-Os5iVz3UuE7abPQNnIuvyBsZc7hVMY0M0_8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=YXRMdc7KiYsDXz9NJeiVHIeoqbY%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:470,&ldquo;originHeight&rdquo;:43,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>decoder vector에 Query를 곱하고 </p><p>encoder vector에 Key를 곱한 후 </p><p>그 둘을 계산하여 Attention score를 계산 이를 softmax로 통과하여 attention distribution 즉, 가중치를 만들어 내고 </p><p>이를 encoder vector의 value들에 곱한다!! </p><p>[##<em>Image|kage@bWv0Qu/btsOMcjF5EO/AAAAAAAAAAAAAAAAAAAAAAhMLglNh1dZVXt4HF_o3G49dac-4pEB-YMPtuY9VRzM/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=zYlrTVAYTUG9f8kFM%2FnQL5qPDZw%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:717,&ldquo;originHeight&rdquo;:373,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:655,&ldquo;height&rdquo;:341}</em>##]</p><h3 id=성능을-봅시다>성능을 봅시다. </h3><p><strong>Machine Translation</strong> </p><p>[##<em>Image|kage@dppGcu/btsOM4d8Xmw/AAAAAAAAAAAAAAAAAAAAACowHO3mblz7Wo1-iI1Hmsj7koe4pZuJDAQPs4JihY85/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=q2ltKQI0%2BhF8IirfD2IfJfixqIg%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:904,&ldquo;originHeight&rdquo;:324,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p><strong>document generation</strong></p><p>[##<em>Image|kage@cH560F/btsOLFmjPYx/AAAAAAAAAAAAAAAAAAAAAOP0FZ61m9K6cZ_N3KXJbor5XqMzgrXt2snrP6Ye7Nwv/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=j8JxrmMrQuHJFhW3LlvSN80Bt5U%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:1068,&ldquo;originHeight&rdquo;:399,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>Transformer는 병렬처리가 잘 되기 때문에 pre-training으로 효율적으로 수행할 수 있게 되었고, 사실상 NLP의 표준이 되었다. </p><h3 id=transformer의-한계>Transformer의 한계 </h3><p><strong>1. Training instabilities (Pre vs Post norm)</strong></p><p>training이 불안정함! 특히 LayerNorm을 어디에 넣느냐에 따라 학습 안정성이 달라짐</p><ul><li><strong>Pre-norm</strong>: LayerNorm을 <strong>Residual 전에</strong> 적용 (요즘 이 방식이 더 안정적임)</li><li><strong>Post-norm</strong>: Residual <strong>후에</strong> 적용 (초기 Transformer 논문 방식)</li></ul><p>[##<em>Image|kage@wZUBY/btsOMjJFANk/AAAAAAAAAAAAAAAAAAAAACaRrxSwZ-2TCwVSz2LENOjyyg5tRODrIAzZ62b7-54O/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=rZUnNPBpCdLtNZdbLYrXb%2BJjxOY%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:986,&ldquo;originHeight&rdquo;:543,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:566,&ldquo;height&rdquo;:312}</em>##]</p><p><strong>2. Quadratic comput in self-attention</strong> </p><p>self-attention 은 계산량이 sequence 길이의 제곱에 비례함 </p><p>왜냐하면</p><p>모든 token쌍마다 attention을 꼐산하기 때문에 O(n^2) 임</p><p>근데 왜 써??? Transformer가 커질 수록 전체 계산에서 self-attention이 차지하는 비중이 점점 줄어든다. 즉 self-attention 계산은 여전히 느리지만, 전체 모델 계산 중 일부일 뿐 </p><p>- 게산을 줄이려고 만든 <strong>저렴한(효율적인) Self-Attention 방법들</strong>은</p><p>→ <strong>모델이 커질수록 성능이 별로 안 좋음.</strong><br>→ 그래서 실제 대형 모델에는 잘 안 쓰인다.</p><p><strong>- Systems optimizations work well (FlashAttention – Jun 2022)</strong></p><p>→ 그래서 <strong>알고리즘을 바꾸는 대신</strong>,<br>→ <strong>기존 Attention을 더 빠르게 구현한 최적화 기술</strong>이 많이 쓰임.</p><h3 id=잠시만-encoder-decoder-구조에서-decoder의-입력이-encoder의-output이-아니었어>잠.시.만 Encoder-Decoder 구조에서 Decoder의 입력이 Encoder의 output이 아니었어???</h3><blockquote><p>✅ <strong>Transformer의 Decoder는 Encoder의 출력을 &ldquo;직접 입력으로 받는 게 아니라&rdquo;, Cross-Attention을 통해 &ldquo;참조"만 해.</strong></p></blockquote><p>즉:<br><strong>Decoder의 입력은 &ldquo;생성 중인 문장"이고</strong>,<br><strong>Encoder의 출력은 &ldquo;참조 대상"일 뿐이야.</strong></p><hr><h2 id=-정리해보자>🔍 정리해보자:</h2><h3 id=transformer-encoder-decoder-구조에서의-역할>Transformer Encoder-Decoder 구조에서의 역할</h3><p>컴포넌트입력처리출력</p><div class=table-wrapper><table><thead><tr><th><strong>Encoder</strong></th><th>전체 source 문장 (ex: 영어)</th><th>Full self-attention</th><th>문장 전체의 의미 벡터</th></tr></thead><tbody><tr><td><strong>Decoder</strong></td><td>지금까지 생성된 target 문장 (ex: 불어 일부)</td><td>Masked self-attention + Cross-attention</td><td>다음 단어 예측</td></tr></tbody></table></div><hr><h2 id=-중요한-구분>📌 중요한 구분</h2><h3 id=-decoder의-입력이-encoder-출력이라는-오해>❌ Decoder의 입력이 &ldquo;Encoder 출력"이라는 오해</h3><ul><li>❌ Decoder는 Encoder의 출력 벡터들을 input으로 &ldquo;넣는&rdquo; 게 아님</li><li>❌ Decoder는 Encoder의 출력을 문장처럼 &ldquo;feeding"하지 않음</li></ul><hr><h3 id=-실제-구조는-이렇게-생겼어>✅ 실제 구조는 이렇게 생겼어:</h3><h4 id=decoder-내부에는-두-개의-attention-layer가-있음>Decoder 내부에는 <strong>두 개의 Attention Layer</strong>가 있음:</h4><ol><li><strong>Masked Self-Attention</strong><br>→ 지금까지 생성한 단어들끼리만 참고<br>→ 미래 단어는 가려져 있음</li><li><strong>Cross-Attention</strong><br>→ Encoder의 출력 전체를 쳐다보면서 필요한 정보 참조<br>→ 즉, <strong>Decoder의 Query가 Encoder의 Key/Value를 &ldquo;attend"하는 구조</strong></li></ol><hr><h2 id=-다시-말해-요약하면>🎯 다시 말해 요약하면</h2><blockquote><p><strong>Decoder는 Encoder의 출력을 input처럼 받는 게 아니라, cross-attention의 대상(memory bank)으로 활용한다.</strong><br>Decoder의 <strong>실제 입력은 지금까지 생성한 target token들</strong>이야.</p></blockquote><hr><h2 id=-시각적으로-그려보면>💡 시각적으로 그려보면:</h2><p>css</p><p>복사편집</p>\[입력 문장 (source)\]<p>→</p>\[Encoder\]<p>→ context representations ↑</p>\[Decoder ← 이전 단어들\]<p>→ Masked self-attention ↓ Cross-attention to Encoder output ↓ 다음 단어 예측 (one token at a time)</p></section><footer class=article-footer><section class=article-tags><a href=/tags/nlp/>NLP</a>
<a href=/tags/deep-learning/>Deep Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/nlp/pretraining/><div class=article-details><h2 class=article-title>Pretraining</h2></div></a></article><article><a href=/p/nlp/transformer-decoder/><div class=article-details><h2 class=article-title>Transformer - Decoder</h2></div></a></article><article><a href=/p/nlp/transformer1-self-attention/><div class=article-details><h2 class=article-title>Transformer(1) - Self-attention</h2></div></a></article><article><a href=/p/nlp/attention/><div class=article-details><h2 class=article-title>Attention</h2></div></a></article><article><a href=/p/nlp/machine-translation-mt%EC%9D%98-%EB%B0%9C%EC%A0%84/><div class=article-details><h2 class=article-title>Machine Translation (MT)의 발전</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 DDubi's World</section><section class=powerby><a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>로 만듦<br><a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>의 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 테마 사용 중</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>