<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="본 글은 cs224n를 정리하였습니다."><title>Transformer(1) - Self-attention</title><link rel=canonical href=https://juijeong8324.github.io/p/nlp/transformer1-self-attention/><link rel=stylesheet href=/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="Transformer(1) - Self-attention"><meta property='og:description' content="본 글은 cs224n를 정리하였습니다."><meta property='og:url' content='https://juijeong8324.github.io/p/nlp/transformer1-self-attention/'><meta property='og:site_name' content="DDubi's World"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='NLP'><meta property='article:tag' content='Deep Learning'><meta property='article:published_time' content='2025-06-21T22:36:00+09:00'><meta property='article:modified_time' content='2025-06-21T22:36:00+09:00'><meta name=twitter:title content="Transformer(1) - Self-attention"><meta name=twitter:description content="본 글은 cs224n를 정리하였습니다."><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_e94bd0a766f2d89e.jpeg width=300 height=235 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>DDubi's World</a></h1><h2 class=site-description>Hello World!</h2></div></header><ol class=menu-social><li><a href=https://github.com/juijeong8324 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>다크 모드</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#self-attention>Self-attention</a></li><li><a href=#keys-queries-values>Keys, Queries, Values</a></li><li><a href=#self-attention의-장벽-그리고-solutions>Self Attention의 장벽.. 그리고 Solutions!</a><ol><li><a href=#1-단어의-순서에-대한-정보가-아예-없잖아--sequence-order><strong>1. 단어의 순서에 대한 정보가 아예 없잖아! : Sequence order</strong></a></li><li><a href=#2-deep-larning을-위한-nonlinearities가-없워-그냥-단순히-weighted-평균이자냐--non-linearities><strong>2. Deep Larning을 위한 nonlinearities가 없워!! 그냥 단순히 weighted 평균이자냐 : non-linearities</strong></a></li><li><a href=#3-sequence를-예측할-때><strong>3. sequence를 예측할 때 “미래를 보지 않는다"는 보장이 있나? : Masking the future in self-attnetion</strong></a></li></ol></li><li><a href=#정리--self-attention을-위한-building-block>정리 : Self-attention을 위한 building block!</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nlp/ style=background-color:#2a9d8f;color:#fff>NLP</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/nlp/transformer1-self-attention/>Transformer(1) - Self-attention</a></h2><h3 class=article-subtitle>본 글은 cs224n를 정리하였습니다.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>6월 21, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>6 분 정도</time></div></footer></div></header><section class=article-content><p>우리가 앞에서 Attention이 input sequence로부터 입력 ht에 중요한 information을 넘겨주는 것이라고 배웠다! </p><p>근데 RNN도 과거 시점의 information을 input로 전달하는 같은 목적이 다! </p><p><strong>그렇다면&mldr; RNN을 굳이 쓸 필요가 있을까?? Attention이 정보를 넘겨주는 방식이라면!</strong> </p><p>그래서 Transformer가 등장!</p><h3 id=self-attention>Self-attention</h3><p>Transformer를 이해하기 위해서는 self-attention을 이해해야 한다. </p><p><strong>Cross Attention</strong>은</p><p>generate yt를 생성하기 위해 input x에 attention하 것! (지난 시간 배운 attention)</p><p>여기서 </p><p><strong>Self-Attention</strong>은</p><p>yt를 생성하기 위해 y&lt;t에 attention하는 것!</p><p>즉, output seq를 생성하는데 현재 시점 t의 출력을 생성할 때 이전 출력들을 보고 생성한다! </p><p>이전 토큰들을 참고해서 중요도를 따지고 이용해서 생성한다! </p><p>[##<em>Image|kage@be8zpa/btsOLD9Pu2C/AAAAAAAAAAAAAAAAAAAAABekskayoAGc-4KwI0taA2S2n5x75RR84yoJp3LmHM5</em>/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=oPXBMgLREoUk%2Bs1lXcOvkMDidx0%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:694,&ldquo;originHeight&rdquo;:431,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}_##]</p><h3 id=keys-queries-values>Keys, Queries, Values</h3><p>자 vocabulary V의 단어 sequence w1:n이 있다고 하자 (e.g Zuko made his uncle tea)</p><p>1. Embedding matrix E에 대하여 word embedding으로 바꾼다. </p><p>각 wi -> xi 로 바꿔준다. (Xi = Ewi)</p><p>2. weight Matrix Q, K, V를 각 word embedding으로 바꾼다. </p><p>[##<em>Image|kage@3bxeM/btsOLCJObQm/AAAAAAAAAAAAAAAAAAAAAPPwJYTdEqOoY-1m_a_89pdr78vQeiFFObvbF_PwsQSB/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=knLrb8DGb9z5D0QLpUUaxb5jFoM%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:704,&ldquo;originHeight&rdquo;:50,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>3. Key와 Query의 similiaites를 계산하고 softmax로 normalize한다! </p><p>[##<em>Image|kage@pu8xq/btsOLhTA3Ic/AAAAAAAAAAAAAAAAAAAAAEdv3PkowQlo4wCjxNURej_xaWUEUbOeP8IDwZKwTaeX/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=EzMQQfvZ6qdz2wZnvtl1PM23nNk%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:440,&ldquo;originHeight&rdquo;:95,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>4. 그것을 가중합하여 계산!~~~</p><p>[##<em>Image|kage@brX2hH/btsOMOCt1Ss/AAAAAAAAAAAAAAAAAAAAAFHwzM5XzmrME2N66kQYaip9Xke9JpxQYgjLwixyGUxc/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=kFWt2skh%2Bd33OGIOUrXlvJKV6B4%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:170,&ldquo;originHeight&rdquo;:89,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><h3 id=self-attention의-장벽-그리고-solutions>Self Attention의 장벽.. 그리고 Solutions!</h3><h4 id=1-단어의-순서에-대한-정보가-아예-없잖아--sequence-order><strong>1. 단어의 순서에 대한 정보가 아예 없잖아! : Sequence order</strong></h4><p>[##<em>Image|kage@ZtbFh/btsOLwJAES9/AAAAAAAAAAAAAAAAAAAAADKFLj7QwjL3cOCIukImqTwR25LakhzG6y-TD18VxGnq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=eJLlud8Oj3YYy2srFT%2F%2F%2BpJp37M%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:473,&ldquo;originHeight&rdquo;:65,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p><strong>Sollutions</strong>: Position representations을 input에 추가해준다!</p><p>즉, key, query, value를 만들기 이전에 input embedding에 order information을 미리 넣어줘야 한다!</p><p><strong>poisional encoding</strong>인 vector Pi 는 pi가 문장의 특정 위치(index)에 있음을 가리킨다. (실제로 sin,cos 로 구성된 고정된 vector 값임)</p><p>[##<em>Image|kage@VkgXq/btsOMP2sD9u/AAAAAAAAAAAAAAAAAAAAAAasu_r12kGGo8RBan-Ib1umxwTvS6JhqtmjjiuDG94M/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=BeQP0E3V%2B7bN4r5dIH3Y33d8z6U%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:529,&ldquo;originHeight&rdquo;:158,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>이를 input embedding에 그냥 더해주면 된다~~ </p><p>그래서 xi~는 위치 정보를 반영한 최종 embedding이므로 </p><p><strong>positioned embedding</strong>, <strong>positional input</strong>이라고 부른다. </p><p><strong>어떻게 Position representation을 만들까?</strong> </p><p><strong>1. Sinusoidal position representations</strong></p><p>[##<em>Image|kage@c9Vr33/btsOLhzgeUc/AAAAAAAAAAAAAAAAAAAAAJavoxqGN70ZHvmnz&ndash;tohf_l3ngojWt1gVCfmWjyhdx/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=bKmPA5WExLRHkchrf7xjRJVyxkI%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:864,&ldquo;originHeight&rdquo;:209,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>sin, cos 함수로 변환해서 벡터로 표현! </p><p>장점</p><p>- vector 차원마다 주기와 dimension이 다르기 때문에 다양한 시점의 패턴을 표현 간으! </p><p>- 완전한 절대적 위치가 아닌 상대적 위치 차이(간격)이 더 중요하게 작동함! </p><p>- 입력 sequence가 길어져도 패턴이 반복될 수 있다! 그래서 안 본 길이까지 일반화(extrapolation)할 수 있음! </p><p>단점</p><p>- Not learnable: 완전히 고정된 함수로 만들어져서, 학습 중에 모델이 자유롭게 조정하거나 최적화 X</p><p>- extrapolation 사실 안됨 : 긴 입력에 대해서 성능이 똑같이 잘 안 나옴&mldr; </p><p><strong>2. scratch(처음부터, 제로로)로 부터 Position representation vectors를 학습시키자!</strong></p><p>Absolute poistion representation을 배우자!</p><p>pi를 학습가능한 parameter도 두자~~ pi는 P matrix의 matrix column으로 두자! </p><p>장점</p><p>- 유연하다. </p><p>단점</p><p>- 일반화 X : 만약 훈련 시 최대 n의 길이인 문장 보았는데&mldr; 그 이상의 문장이 들어오면?? 안 됨</p><p><strong>3. RoPE : Common, modern position embeddings</strong></p><ul><li><strong>다시 생각해보자.</strong> </li></ul><p>positioned embedding을 f(x, i) 형태로 relative position embedding으로서 생각해보자. </p><p>f(x, i) : 단어 x의 위치 i에서의 embedding이라고 해보자  (x는 단어, i는 위치)</p><p>[##<em>Image|kage@bmsHwM/btsOMo48Apu/AAAAAAAAAAAAAAAAAAAAABCTMEWbF7JEplWdLSExxE0LJ4etKHholRd305xf5yI2/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=2tUd%2F%2FGuyVAjpc2GWv3hhXMOQY0%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:346,&ldquo;originHeight&rdquo;:79,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>여기서 Attention function g는  x, y의 의미와 (i-j)라는 상대적 위치 차이에만 의존해야 한다!!</p><p>기존의 positional embedding은 이 목표를 충족시키지 못 하는가? </p><p>- 1의 경우 : 상대적이지 않은 다양한 cross-term을 가진다. (절대 위치 정보가 따로따로 반영된 것) </p><p>- 2의 경우 : (i-j)에 대하여 inner product로 표현되지 않는다.</p><ul><li> <strong>Embedding via rotation</strong></li></ul><p><strong>우리는 position embedding이 absolute position에 영향을 받지 않게 하고 싶음! -> relative position으로!</strong> </p><p><strong>우리는 inner products가 arbitrary rotation에 변하지 않는다는 것을 알고 있어! -> 내적 값은 동일해!</strong> </p><p>[##<em>Image|kage@DtFFE/btsOLSr9vWn/AAAAAAAAAAAAAAAAAAAAADePNAOx6AlQof92jWFhowuivem3neJo5rh3uBiwXpN8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=Q0rZ6D1%2F5q6iS2VU%2Fu6AMsCUFts%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:829,&ldquo;originHeight&rdquo;:306,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:677,&ldquo;height&rdquo;:250}</em>##]</p><p><strong>내적 기반으로 attention score를 계산하면..! 상대 위치 기반 attention이 가능하다!</strong> </p><p>[##<em>Image|kage@1fMUn/btsOKA7fddu/AAAAAAAAAAAAAAAAAAAAAFiY4bPdxRyyNhxboOvoLfxVa_NpvRX9zY2gSX9qV884/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=0Ypa%2B5YtdisnSDC%2BdvW7G6TNGCQ%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:766,&ldquo;originHeight&rdquo;:439,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p>따라서 coordinate를 짝지어서 2D 평면에서 회전시켜라!!! (아이디어는 복소수에서 따온 것!!) </p><p>즉 위치 i에 대하여 단어 vector x에 대해서 sin/cos을 곱하여 벡터 공간에서 회전시키자! </p><p>위의 그림을 보면 </p><p>각 단어는 d 차원의 Query/Key 벡터를 가진다! 그리고 poision이 적용 된 후 Position embdding(위치 정보가 내제된) Query/Key가 생성된다. </p><p>단어 Enhanced에 대한 Query와 Key vector (x1, x2)가 존재하고 position m(index임)이 있다.</p><p>이때 Q, K vector(x1, x2)를 sin/cos 기반으로 m만큼 회전시킨다. 그러면 반영이 되고!! </p><p>그러면!! 회전이 반영된 vector된 후 Enhanced라는 Q 벡터(위치 1)와 Rotary라는 단어의 K 벡터(위치 4)가 dot product하여 attention score 계산 시 위치 정보가 반영이 된다~~~ </p><h4 id=2-deep-larning을-위한-nonlinearities가-없워-그냥-단순히-weighted-평균이자냐--non-linearities><strong>2. Deep Larning을 위한 nonlinearities가 없워!! 그냥 단순히 weighted 평균이자냐 : non-linearities</strong></h4><p>[##<em>Image|kage@cnQ8RS/btsOLBKXrNm/AAAAAAAAAAAAAAAAAAAAAFKU9q_Lzhwt1sEmCWN69mWDpMJBFc3f2Bne5KxbOL6F/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=PH05iw82z83oI%2Bz%2FWBPsUqLeBgQ%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:684,&ldquo;originHeight&rdquo;:530,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:506,&ldquo;height&rdquo;:392}</em>##]</p><p>Self-attention 메커니즘 자체에는 비선형성 이 없다&mldr; 전부 선형 연산임!!</p><p>따라서 Self-attention만 계속 쌓다보면</p><ul><li>이전 layer에서 얻은 Value vector들을 다른 방식으로 <strong>재평균(re-averaging)</strong> 하는 것에 불과하게 됨</li></ul><p><strong>Solutions</strong>: Self-attention 이후 Feed-Forward network로 비선형성을 추가하여 각 output vector를 처리함! </p><p>[##<em>Image|kage@bC9JFW/btsOMaTAZXU/AAAAAAAAAAAAAAAAAAAAAIMhHk4CCtIMmh3ZLczQgxLDfGQkZneu9oxm_6RbHjjj/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=ZwTxvW9sZnvB1u21ip%2FiGgjZ8wk%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:492,&ldquo;originHeight&rdquo;:141,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><h4 id=3-sequence를-예측할-때><strong>3. sequence를 예측할 때 &ldquo;미래를 보지 않는다"는 보장이 있나? : Masking the future in self-attnetion</strong></h4><p>이 말은 즉슨, Machine translation이나 Language modeling과 같이 순차적인 task에서 아직 나오지 않은 단어를 미리 참고하면 안 된다는 뜻!!! (self-attention은 input 전체를 확인하니까!) -> 즉, training 할 때! (inference는 아님!!) </p><p>[##<em>Image|kage@ddgVDd/btsOKS0PxOh/AAAAAAAAAAAAAAAAAAAAAOi-yB3vie78rxzMKs93S6BqZjHFUgrvqUUa1bHpPxsR/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=cyvxv8gcPjAo1aOSFR5x51ViWMo%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:704,&ldquo;originHeight&rdquo;:298,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:324,&ldquo;height&rdquo;:137}</em>##]</p><p>Decoder는 기본적으로 self-attention과 cross-attention을 사용한는데,</p><p>train 도중에 input과 target을 모두 받는다. 이때 단어 예측하는데 self-attention 시 target seq 전체를 참고할 수 있으니, 이를 가려야 함.</p><p>그러면 매 timestep마다 keys들과 querys를 바꿔서 이전 단어만 보게 만들면 될까? -> Inefficient!</p><p>매 timestep마다 attention 범위를 다시 설정해야 하므로 병렬 계산이 불가능&mldr; </p><p>즉, paralleization이 가능하려면</p><p>- 전체 seq를 다 넣은 후</p><p>- future 단어들의 attention scores를 -무한대로 설정하여 attention을 mask out 처리해야 함! </p><p>- softmax에서 exp (−∞ ) = 0이므로 완전히 무시된다~~</p><p>[##<em>Image|kage@TRTZW/btsOLhF2Z4s/AAAAAAAAAAAAAAAAAAAAADuKmel_ldl2RlpnDJcyRSNAbSZzMBz7mppbBHytCNAl/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=9kk6y0Xu1gbYNWN3q28meDty3dY%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:240,&ldquo;originHeight&rdquo;:93,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##][##<em>Image|kage@baAlZ0/btsONayxgTE/AAAAAAAAAAAAAAAAAAAAANjvvtTqGeILUDd_ZK6OgOMF1lUF9iu1TwhfAOM-A1J5/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=bow2jZu5lSfPkPtaH5X1Y6%2F0soM%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:729,&ldquo;originHeight&rdquo;:587,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;,&ldquo;width&rdquo;:507,&ldquo;height&rdquo;:408}</em>##]</p><p><strong>Solutions</strong>: attention weight를 0으로 일부러 만들어서 미래를 mask out한다. </p><h3 id=정리--self-attention을-위한-building-block>정리 : Self-attention을 위한 building block!</h3><p>[##<em>Image|kage@brSmTy/btsOKUdjH2z/AAAAAAAAAAAAAAAAAAAAAMg4S7fYAb3shTEAmgIlyP-GYvVUNOoPsSiBoIg6-Gct/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&amp;expires=1761922799&amp;allow_ip=&amp;allow_referer=&amp;signature=oG%2FancrmYQbbA1RrMiYyw7MZXLc%3D|CDM|1.3|{&ldquo;originWidth&rdquo;:405,&ldquo;originHeight&rdquo;:605,&ldquo;style&rdquo;:&ldquo;alignCenter&rdquo;}</em>##]</p><p><strong>1. self-attention</strong> </p><p><strong>2. Position representations</strong></p><p>seq oreder를 구체화하기 위해서! self-attention은 input의 순서를 고려하지 않기 때문에! </p><p><strong>3. nonlineariies</strong></p><p>self-attention block의 output에 Feed Forwar newrok를 구현!</p><p><strong>4. Masking</strong></p><p>미래를 보지 않고 parallize 연산을 하기 위해서 </p><ul><li>미래에 생성돼야 할 단어의 정보가</li><li><strong>앞 단어 예측 과정에 흘러 들어가면 안 됨</strong> ❌</li><li>Masking은 이런 **정보 누출(leakage)**을 막아줌</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/nlp/>NLP</a>
<a href=/tags/deep-learning/>Deep Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/nlp/pretraining/><div class=article-details><h2 class=article-title>Pretraining</h2></div></a></article><article><a href=/p/nlp/transformer-encoder/><div class=article-details><h2 class=article-title>Transformer - Encoder</h2></div></a></article><article><a href=/p/nlp/transformer-decoder/><div class=article-details><h2 class=article-title>Transformer - Decoder</h2></div></a></article><article><a href=/p/nlp/attention/><div class=article-details><h2 class=article-title>Attention</h2></div></a></article><article><a href=/p/nlp/machine-translation-mt%EC%9D%98-%EB%B0%9C%EC%A0%84/><div class=article-details><h2 class=article-title>Machine Translation (MT)의 발전</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 DDubi's World</section><section class=powerby><a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>로 만듦<br><a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>의 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 테마 사용 중</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>