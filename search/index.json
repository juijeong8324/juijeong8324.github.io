[{"content":"1. Subword modeling 간단하게 알아보기 [##Image|kage@bO6314/btsO5L1bTW2/AAAAAAAAAAAAAAAAAAAAAGRPfPUE5tQq1W_Qa92a8ZA-rJdGPLmE4vFSLj_KMfxw/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=xr0cD%2FJl2O0ronELf5POn%2FTFJqs%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:1046,\u0026ldquo;originHeight\u0026rdquo;:319,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n먼저, 우리의 language의 vocabulary를 만들 때 다음과 가정을 한다고 하자. training set으로 부터 만들어진, 수 천개의 words로 이루어진 fixed vocab가 있고,\ntest time에 처음 본 모든 word들에 대해서 single UNK로 mapping 한다고 가정한다. 이 상황에서, subword의 modeling은 word level보다 더 작은 structure (Parts of words, characters, bytes) 를 고려하는 다양한 method 방법들을 포함한다! 현대 NLP에서는 subword tokens으로 구성된 vocabulary를 학습하는 방식이 지배적이다. training과 testing 시 각 word는 vocabulary에 알려진 subword들의 sequence로 분할된다. The byte-pair encoding(BPE) algorithm 이때, BPE는 subword vocabulary를 정의하기 위한 effective strategy 라고 할 수 있다!! 1. init vocabulary : 모든 문자를 각 character들과 단어 끝(end-of-word) symobl만 포함\n2. text의 corpus를 보면서 가장 자주 함께 등장하는 character 쌍 \u0026ldquo;a, b\u0026quot;를 찾아서 \u0026ldquo;ab\u0026quot;를 subword로 추가 3. 해당 charcter pair를 new subword로 replace, 이때 vocab size로 도달할 때까지 반복!!! 근데 이 방식은.. 초기 machine translation과 같은 NLP에만 사용되었음!! 현재는 WordPiece(확률 based) 와 같은 method가 pretrained model에서 사용되고 있음!!1 - WordPiece?? byte coding 자세한 건\n[##Image|kage@bgcmpl/btsO6rA7437/AAAAAAAAAAAAAAAAAAAAAN1b4Om0TbnQVPR2anMt7BwNRYiZ1lLCWBr69jOQT1YB/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=zlcbvOh1sPsfw%2FXHI3cJtTFwdCU%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:738,\u0026ldquo;originHeight\u0026rdquo;:223,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n대부분의 word들은 subword vocabulary의 일부에 포함되겠지만,, 드문 word들은 component로 분리된다\u0026hellip;\n아주 안 좋은 경우에는 word가 아주 많은 subwords로 분리될 수도 있다는 사실\u0026hellip; 2. Motivating model pretraining from word embeddings 그래서..! 4. Model pretraining three ways 4-1. Decoders\n4-2. Encoders\n4-3. Encoder-Decoders\n5. Interlude: what do we think pretraining is teaching? 6. Very Large models and in-context learning ","date":"2025-07-06T19:14:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/pretraining/","title":"Pretraining"},{"content":"[##Image|kage@ts3IR/btsOMTqkBt1/AAAAAAAAAAAAAAAAAAAAAO5eCa8c7NYTnezb7sEzqKBacKSPresrBDlEMbGfNoLD/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=W679xjbXXwiqbkktqAK99VZUDmE%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:602,\u0026ldquo;originHeight\u0026rdquo;:667,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:425,\u0026ldquo;height\u0026rdquo;:471}##]\nTransformer Decoder는 **language model처럼 단방향(unidirectional) context만을 사용하도록 contraint(제한)**한다. -\u0026gt; 즉 과거(왼쪽) 정보만 보고 미래(오른쪽)는 보지 않는다. -\u0026gt; 이는 Langue model처럼 다음에 올 단어를 예측하는 task\n그러면 우리는 bidirectional context를 원하면 양방향 RNN처럼 하면 될까? 그럴 때 사용하는 것이!!!!! Transformer Encoder이다!\n-\u0026gt; 이는 문장 이해, 분류에 적합한 task Decoder와의 유일한 차이점은 self-attention에서 masking을 제거한다는 것..! (즉, 모든 단어를 동시에 본다!) The transformer Encoder-Decoder [##Image|kage@EsUBF/btsOMe2P77d/AAAAAAAAAAAAAAAAAAAAAMMZMPLAgERAICX1MeRLDaUvqrufJ3FCTwJ4Acj8HCpg/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=lSVxZmTQSeho1FJ6UDsGGzTTBM0%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:648,\u0026ldquo;originHeight\u0026rdquo;:694,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:381,\u0026ldquo;height\u0026rdquo;:408}##]\nMachine translation을 회기하면,, 우리는 bidirectional model에서 source sentence를 처리했고, unidirectional model에서 target을 생성했다! seq2seq format의 작업에서는\u0026hellip; 이렇게 Transformer Encoder-Decoder 구조를 사용한다!!!!!\nEncoder\n일반적인 Transformer Encoder를 그대로 사용 Decoder\nEncoder의 출력에 대해 cross-attention을 수행할 수 있도록 수정된다. Decoder는 self-attention으로 자기 자신만 보는게 아니라, Encoder가 만든 input 문장의 context를 참고해서 번역을 생성 Cross-attention (details) [##Image|kage@c0gxt1/btsOMku0Kfk/AAAAAAAAAAAAAAAAAAAAABG7eNvJgk9_dlDwwjsrY0PNbkeQG-hXvdGsIMMsGfdq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=Wz7T1TPA5m%2BSrkRUCGrk21%2FpPg4%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:597,\u0026ldquo;originHeight\u0026rdquo;:482,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:382,\u0026ldquo;height\u0026rdquo;:308}##]\nself-attention은 같은 source에서 keys, queries, values가 만들어질 때를 말한다.\n즉, Encoder 내부, Decoder 내부 Decoder에서는 self-attention도 있지만, 다른 source를 바라보는 attention도 있다!(저번 시간에 배움) 그게 바로 cross-attention\nh1, \u0026hellip; , hn: Transformer encoder 로 부터 얻은 output vector(hidden state) 𝑥𝑖∈ℝ𝑑 z1, \u0026hellip; , zn: Transformer decoder로 부터 얻은 input vector(즉, 이미 생성된 단어들의 hidden state) 𝑧𝑖∈ℝ𝑑 여기서 keys와 values는 encoder의 출력 hi에서 계산\n즉 Encoder는 일종의 memory 같은 느낌 𝑘𝑖=𝐾ℎ𝑖, 𝑣𝑖=𝑉ℎ𝑖\nqueries는 decoder로 현재 입력 zi에서 가져온 것 𝑞𝑖=𝑄𝑧𝑖.\n그니까 정리하자면\u0026hellip; query는 decoder의 input 우리가 집중해서 보고 싶은 것들은 encoder의 input이고, key와 value에 해당 즉 Decoder가 Encoder의 출력에 대해 attention 을 날리는 것! 참조하며 문장을 생성할 수 있게!!\nCross-attention이 계산되는 과정 [##Image|kage@leV61/btsOLl2zPW4/AAAAAAAAAAAAAAAAAAAAAIA7cXyO2OQFSby4v8-pcRp2XmxzWFnaRyUMSH9ihgPv/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=V2xFp9KXOuxlWZ8yLjJQZxeZ4EI%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:309,\u0026ldquo;originHeight\u0026rdquo;:88,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nH는 encoder vector의 concatenaton! Z는 dcoder vector의 concatenation!\n여기서 T는 encoder의 길이 또는 decoder의 길이 중 하나일 수 있는데, 문맥상 둘 다 T로 표기한 거지 실제로는 다를 수 있어.\n예를 들어: 영어 입력(5단어) → 프랑스어 출력(7단어)이라면 Encoder: T₁ = 5, Decoder: T₂ = 7 [##Image|kage@cgCHrb/btsOMFyVJLj/AAAAAAAAAAAAAAAAAAAAAAfCTekS-Os5iVz3UuE7abPQNnIuvyBsZc7hVMY0M0_8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=YXRMdc7KiYsDXz9NJeiVHIeoqbY%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:470,\u0026ldquo;originHeight\u0026rdquo;:43,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\ndecoder vector에 Query를 곱하고 encoder vector에 Key를 곱한 후 그 둘을 계산하여 Attention score를 계산 이를 softmax로 통과하여 attention distribution 즉, 가중치를 만들어 내고 이를 encoder vector의 value들에 곱한다!! [##Image|kage@bWv0Qu/btsOMcjF5EO/AAAAAAAAAAAAAAAAAAAAAAhMLglNh1dZVXt4HF_o3G49dac-4pEB-YMPtuY9VRzM/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=zYlrTVAYTUG9f8kFM%2FnQL5qPDZw%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:717,\u0026ldquo;originHeight\u0026rdquo;:373,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:655,\u0026ldquo;height\u0026rdquo;:341}##]\n성능을 봅시다. Machine Translation [##Image|kage@dppGcu/btsOM4d8Xmw/AAAAAAAAAAAAAAAAAAAAACowHO3mblz7Wo1-iI1Hmsj7koe4pZuJDAQPs4JihY85/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=q2ltKQI0%2BhF8IirfD2IfJfixqIg%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:904,\u0026ldquo;originHeight\u0026rdquo;:324,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\ndocument generation\n[##Image|kage@cH560F/btsOLFmjPYx/AAAAAAAAAAAAAAAAAAAAAOP0FZ61m9K6cZ_N3KXJbor5XqMzgrXt2snrP6Ye7Nwv/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=j8JxrmMrQuHJFhW3LlvSN80Bt5U%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:1068,\u0026ldquo;originHeight\u0026rdquo;:399,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nTransformer는 병렬처리가 잘 되기 때문에 pre-training으로 효율적으로 수행할 수 있게 되었고, 사실상 NLP의 표준이 되었다. Transformer의 한계 1. Training instabilities (Pre vs Post norm)\ntraining이 불안정함! 특히 LayerNorm을 어디에 넣느냐에 따라 학습 안정성이 달라짐\nPre-norm: LayerNorm을 Residual 전에 적용 (요즘 이 방식이 더 안정적임) Post-norm: Residual 후에 적용 (초기 Transformer 논문 방식) [##Image|kage@wZUBY/btsOMjJFANk/AAAAAAAAAAAAAAAAAAAAACaRrxSwZ-2TCwVSz2LENOjyyg5tRODrIAzZ62b7-54O/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=rZUnNPBpCdLtNZdbLYrXb%2BJjxOY%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:986,\u0026ldquo;originHeight\u0026rdquo;:543,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:566,\u0026ldquo;height\u0026rdquo;:312}##]\n2. Quadratic comput in self-attention self-attention 은 계산량이 sequence 길이의 제곱에 비례함 왜냐하면\n모든 token쌍마다 attention을 꼐산하기 때문에 O(n^2) 임\n근데 왜 써??? Transformer가 커질 수록 전체 계산에서 self-attention이 차지하는 비중이 점점 줄어든다. 즉 self-attention 계산은 여전히 느리지만, 전체 모델 계산 중 일부일 뿐 - 게산을 줄이려고 만든 저렴한(효율적인) Self-Attention 방법들은\n→ 모델이 커질수록 성능이 별로 안 좋음.\n→ 그래서 실제 대형 모델에는 잘 안 쓰인다.\n- Systems optimizations work well (FlashAttention – Jun 2022)\n→ 그래서 알고리즘을 바꾸는 대신,\n→ 기존 Attention을 더 빠르게 구현한 최적화 기술이 많이 쓰임.\n잠.시.만 Encoder-Decoder 구조에서 Decoder의 입력이 Encoder의 output이 아니었어??? ✅ Transformer의 Decoder는 Encoder의 출력을 \u0026ldquo;직접 입력으로 받는 게 아니라\u0026rdquo;, Cross-Attention을 통해 \u0026ldquo;참조\u0026quot;만 해.\n즉:\nDecoder의 입력은 \u0026ldquo;생성 중인 문장\u0026quot;이고,\nEncoder의 출력은 \u0026ldquo;참조 대상\u0026quot;일 뿐이야.\n🔍 정리해보자: Transformer Encoder-Decoder 구조에서의 역할 컴포넌트입력처리출력\nEncoder 전체 source 문장 (ex: 영어) Full self-attention 문장 전체의 의미 벡터 Decoder 지금까지 생성된 target 문장 (ex: 불어 일부) Masked self-attention + Cross-attention 다음 단어 예측 📌 중요한 구분 ❌ Decoder의 입력이 \u0026ldquo;Encoder 출력\u0026quot;이라는 오해 ❌ Decoder는 Encoder의 출력 벡터들을 input으로 \u0026ldquo;넣는\u0026rdquo; 게 아님 ❌ Decoder는 Encoder의 출력을 문장처럼 \u0026ldquo;feeding\u0026quot;하지 않음 ✅ 실제 구조는 이렇게 생겼어: Decoder 내부에는 두 개의 Attention Layer가 있음: Masked Self-Attention\n→ 지금까지 생성한 단어들끼리만 참고\n→ 미래 단어는 가려져 있음 Cross-Attention\n→ Encoder의 출력 전체를 쳐다보면서 필요한 정보 참조\n→ 즉, Decoder의 Query가 Encoder의 Key/Value를 \u0026ldquo;attend\u0026quot;하는 구조 🎯 다시 말해 요약하면 Decoder는 Encoder의 출력을 input처럼 받는 게 아니라, cross-attention의 대상(memory bank)으로 활용한다.\nDecoder의 실제 입력은 지금까지 생성한 target token들이야.\n💡 시각적으로 그려보면: css\n복사편집\n\\[입력 문장 (source)\\] → \\[Encoder\\] → context representations ↑ \\[Decoder ← 이전 단어들\\] → Masked self-attention ↓ Cross-attention to Encoder output ↓ 다음 단어 예측 (one token at a time)\n","date":"2025-06-22T15:45:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/transformer-encoder/","title":"Transformer - Encoder"},{"content":"Transformer는 Decoder와 Encoder 구조로 이루어져 있다..! 앞에서 배운 핵심 개념들을 바탕으로 각 구조를 자세히 알아보도록 하자. Transformer Decoder [##Image|kage@bU8qFf/btsOKCcUETl/AAAAAAAAAAAAAAAAAAAAANxHlKSJS4OWqoujltyGRQZCjz-0SeP4dPPdkvWSJj4Q/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=eYzKHUr9ewovgtCNHcXFdQL0lSQ%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:426,\u0026ldquo;originHeight\u0026rdquo;:594,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:255,\u0026ldquo;height\u0026rdquo;:356}##]\n쉽게 생각해서 Language models처럼 어떻게 우리가 systems을 build할까를 생각하면 된다. minimal self-attention architecture로 보이지만, 사실 좀 더 많은 component가 있다! - Embeddings와 Position Embeddings는 동일하다. 1. Multi-head Attention 먼저 self-attention이 아닌 multi-head self-attetion이네..?\n[##Image|kage@JZfkN/btsOLeP8Om5/AAAAAAAAAAAAAAAAAAAAAG8vKO-oE3BZoqxfdNLP0Uk8-Drz3pCbja42a7Rje257/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=FxRnVrMw6ZV7WPJmfY2nPPglNQg%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:1048,\u0026ldquo;originHeight\u0026rdquo;:528,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:775,\u0026ldquo;height\u0026rdquo;:390}##]\n여기서 알 수 있는 사실.. Head는 무엇을 attention 할 것인지를 결정해주는 것이라고 할 수 있다. 우선\u0026hellip; Attention이 계산되는 과정을 알아보자! Matrices에서 key-query-value attention이 어떻게 계산되는 지 봅시다\u0026hellip;\nInputInput vectors의 concatenation라고 할 수 있다. X = \\[x1; ... ; xn\\] ∈ ℝ𝑛× 𝑑 (행이 n개, 열이 d개; 즉, n은 단어의 개수 d는 차원) Output\nsoftmax(XQ(XK)T)XV ∈∈ℝ𝑛×𝑑\n이때, 𝑋𝐾 ∈ ℝ𝑛×𝑑, 𝑋𝑄 ∈ ℝ𝑛×𝑑, 𝑋𝑉 ∈ ℝ𝑛×𝑑 1. Query-Key dot products : 𝑋𝑄 𝑋𝐾 ⊤\n[##Image|kage@bHygVe/btsOMHpR4u6/AAAAAAAAAAAAAAAAAAAAAJQHdXe472r-kBNk9-TJICHeLE0lO-blGYRs7X__jwwM/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=27m1ZJXKwrOLB5luicgVpt2yEcU%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:668,\u0026ldquo;originHeight\u0026rdquo;:172,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n즉 결과는 쉽게 생각해서 각 Query 벡터와 각 Key 벡터 간의 dot product의 값을 모두 구한 matrix라고 보면 된다..! 결과 matrix의 (i,j)의 값은 Q의 i번째 행(i번째 word의 Query) K의 j번째 열(j번째 word의 Key)의 dot product (현재 Query가 Key를 얼마나 참고하는가?) 라고 보면 된다!! 그리고한 행이 Query에 대한 다른 key의 attention score가 된다. [##Image|kage@XI3Hz/btsOKK9wXI7/AAAAAAAAAAAAAAAAAAAAAMmmaeiAuZCsfruzcRqCSsGtNYOyLw-ytHZSWZgzwZ_u/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=nFFGzsI2B7YglWAhS%2FfDo59dIzw%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:684,\u0026ldquo;originHeight\u0026rdquo;:194,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n그리고 softmax를 적용하먄,\n각 행은 Query 위치 i가 전체 Key들(j=1~n)에 대해 attention하는 가중치\n이를 V(각 word의 실제 정보)에 곱하여 weighted average를 계산한다! softmax 결과의 한 행 (즉, Query가 보는 attention weight)\n와 V의 한 열 (특정 의미 차원만 담고 있음)을 곱하면\n과연 이게 무슨 의미지?”\nSoftmax한 한 행 × V의 한 열 =\n\u0026ldquo;이 Query가 해당 의미 차원에 대해 얼마나 정보를 끌어왔는가?\u0026rdquo;\n그리고 그걸 d_v 개 차원에 대해 모두 계산하니까\n→ i번째 Query의 context vector가 나오는 거예요!\n즉 결과 matrix의 (i, j)의 값은 한 word의 Attention 가중치에 대하여 다른 word들에 대해 곱하여 더한 값(이때 다른 word의 일부 차원만 곱한 것) 따라서 i번째 행은 전체 context임!!\n만약 우리가 한 sentence에서 여러 위치를 한번에 보고싶다면 어떨까??\n단어 i는 보통, self-attention이 xiTQTKxj (즉 xi의 Query, xj의 Key)가 높은 곳을 \u0026ldquo;보고\u0026quot;있는데, 즉 단어 j를 바라보는데, 어쩌면 다른 이유로 다른 j(다른 단어들)도 집중해서 보고 싶을 수도 있지 않을까?\n우리는 여러 개의 Q, K, V metrices을 사용해서 여러 개의 attention \u0026ldquo;head\u0026quot;를 정의할 것이다. [##Image|kage@Q6rwO/btsOKByllUD/AAAAAAAAAAAAAAAAAAAAAAWQ59oYeD3MYB6pbl_b33hwnPTT02HUEnPvMDQSSXSL/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=8mCDlJK7KD%2FYybjO0feQnPsIxrM%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:217,\u0026ldquo;originHeight\u0026rdquo;:50,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nQl, Kl, Vl는 각각 l번째 head에서 사용되는 행렬이고,\nh는 head의 개수, l은 head 번호 (1부터 h까지)라고 하자. 여기서 전체 차원 d를 head 수 h로 나누어서 각 head는 더 작고 독립적인 공간에서 작동함.\n[##Image|kage@dC2ypW/btsOLuSGwUr/AAAAAAAAAAAAAAAAAAAAAJHiJj9SOuemqas0W38uXz5iwcuUmErvBcFPS7gdwi_u/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=I8M1ZFDFf1HnsKa3x8l9Z7oHzSI%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:814,\u0026ldquo;originHeight\u0026rdquo;:57,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n각 attention head는 attention은 독립적으로 수행한다. attention score : XQl(Kl)TXT\nvalue 합산 : 그 결과에 XVl를 곱합 각 head는 서로 다른 방식으로 attention을 수행하고, 그 결과값은 작지만 독립적인 feature representation이 된다. [##Image|kage@JEEE9/btsOMVuVg3F/AAAAAAAAAAAAAAAAAAAAANwwdgS3eIQEJWHparx\u0026ndash;n0sZBjcxZZN937ht1nKoYGE/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=k0QKofMYuIO4QAUiYNiAbzg9h%2BA%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:660,\u0026ldquo;originHeight\u0026rdquo;:42,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\noutput vector를 행렬로 만든 후 combine 즉, 각 head는 서로 different things를 \u0026ldquo;보고\u0026rdquo;, value vectors도 다르게 constrcut한다!!\n효율적으로 계산하는 방법 우리가 h개의 많은 attention heads를 계산할지라도 그것은 매우 비싸지 않다. 1. 𝑋𝑄∈ℝ𝑛×𝑑 를 계산하고 ℝ𝑛×ℎ×𝑑/ℎ 로 reshape 한다. (XK, XV도 마찬가지로!!) 이렇게 되면 d차원의 n개의 word에서 attention head h개를 만들고 싶기 때문에\ndimenstion d를 h개의 head로 나누고 각 head에 d/h만큼 할당해준다. 따라서 resahpe 시 각 단어(n)개가 h개의 head를 가지며 각 head는 d/h 차원의 query vector를 가진다. (2차원 행렬이었는데 이제 3차원 tensor가 된다) 2. ℝℎ×𝑛×𝑑/ℎ 로 transpose한다. 그러면 head axis이 batch axis처럼 작동한다. (또한 맨 앞이 보통 depth의 의미..!)\nTransformer 구현에서는 각 head에 대해 독립적으로 attention을 계산해야 하는데, 보통 batch aixs를 제일 앞에 두고 batch 차원 단위로 병렬 연산을 수행한다. 즉, 각 head를 독립적인 sample처럼 병렬 처리 하기 위해서다! 그냥 쉽게 생각해서 h개의 head가 nxd/h의 크기의 matrix를 갖고 있다 생각하자!\n1 2 3 4 5 6 7 8 원래: [ 단어1: head1, head2, ..., headh ] [ 단어2: head1, head2, ..., headh ] ↓ transpose [ head1: 단어1, 단어2, ... ] [ head2: 단어1, 단어2, ... ] ... 거의 다른 모든 것은 동일하고 행렬들의 크기들도 동일하다!! [##Image|kage@bqj8M3/btsOMWHmfti/AAAAAAAAAAAAAAAAAAAAAF57waTz0TN_gIlpr1n2S9W0yFIumLkPrqiVmgsA3rWk/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=paBWHF47CiK5BkOGc1zDSSZ6G68%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:712,\u0026ldquo;originHeight\u0026rdquo;:205,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n위의 그림을 보면 head가 3인 것을 알 수 있다. (가로 세로가 차원과 대응되는게 아님 주의!!!) XQ, XK: nxd/h가 3개 있다! 라고 이해하면 쉬울듯! XQKX: 3개의 head가 각각 nxn 크기의 attention score matrix를 갖고 있다. dot product해서 각 head의 attention score를 계산하게 된다. [##Image|kage@Rthvu/btsONayBQbW/AAAAAAAAAAAAAAAAAAAAAAnSO07E_Wm7qCPAZthbcpzc44sdrbX8s1TVz2gLUm1n/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=Ndh%2FrOW7XNRiZ3xjeSy3rRphUp4%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:808,\u0026ldquo;originHeight\u0026rdquo;:193,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nsoftmax를 수행한 후 각 head별로 weighted average 수행한다. 각 head의 attention scores matrix와 각 head의 XV를 각각 곱한 결과라고 생각하면 된다!\nScaled dot Product Q와 K의 차원이 커지면 내적 값이 너무 커져서 softmax가 터지니까, 그걸 방지하려고 √(d/h)로 나눠주는 게 “Scaled Dot Product Attention”이다.\n만약 dimensionality d (embedding 차원의 수)가 너무 커지면, vector 간의 dot products 값도 더 커지는 경향이 있다. 이러면 softmax 함수의 input이 더 커질 수 있는데,,, 그러면 softmax의 출력이 너무 뾰죡해져서, backpropagation 때 gradients가 매우 작아지는 문제가 생긴다.\n우리가 본 이 self-attention function 대신에\n[##Image|kage@dFKBY5/btsONaSUJ9p/AAAAAAAAAAAAAAAAAAAAAInbOgy7vn8xxuJxnVytPDmpeOGYo4fkla18Rq2j3CDd/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=gkmQXl51aS%2BHeswq2915ngwBC5Q%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:483,\u0026ldquo;originHeight\u0026rdquo;:58,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nattention score들을 루트 d/h로 나누어서 scores들이 더 커지지 않게 막아준다! [##Image|kage@3OYaJ/btsOMPnVrZE/AAAAAAAAAAAAAAAAAAAAAB1UXP4AA7e0w5fsq_VPe6CoDIZ5mmgqYX1BeOgiruWF/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=el2wnpDwXb2wRhpEpczmk68PBXY%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:471,\u0026ldquo;originHeight\u0026rdquo;:69,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##][##Image|kage@o4J0S/btsONd3buUS/AAAAAAAAAAAAAAAAAAAAADofAExHVj7_N97OY9Zs-8Lij24z1s5Pck0KkqGBpciL/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=eiqFHJqOS1ZNOlMLDswQfIlnhQM%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:371,\u0026ldquo;originHeight\u0026rdquo;:567,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n2. Residual Connections 이제 optimization trick 몇 가지를 알아보자\u0026hellip;!!! 이전 결과와 Layer를 통과한 결과를 더함으로써 그 변화량만 학습한다!! 보통 diagram에서는 종종 Add \u0026amp; Norm이라고 표현되므로 참고하시길..! Residual Connections은 model 더 잘 traing되도록 도와주는 trick이다. [##Image|kage@bHgXRo/btsONbK3W44/AAAAAAAAAAAAAAAAAAAAAKOrF8OaH0_iKARp0hS_5yMMgOV7Pd2q4rsEMUzg0uMI/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=QS9Id%2FLagGTDlT%2BMxSioY2%2FW4Xc%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:437,\u0026ldquo;originHeight\u0026rdquo;:88,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n우리는 보통 i번째 layer의 representaion은 이전 layer의 representation을 layer를 넣은 결과라고 생각한다.\n[##Image|kage@x53sD/btsOLdKt4Gt/AAAAAAAAAAAAAAAAAAAAAFF5A4DLuQQoRHBwsKR6Sfztkwdhx6gK70aJevsNkdmC/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=9R764qZ%2FKZsn6qp0WZAq%2Fg5jZjw%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:430,\u0026ldquo;originHeight\u0026rdquo;:96,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n그런데, Residual Connection에서는 위와 같이 바꿔진다. 이전 layer의 representation에 Layer의 representation을 더한다!\n즉, 우리는 전체 function를 새로 배우는 게 아니라 residual(변화량)만 학습하면 된다!! 즉 기존 입력 + 변화된 부분을 넣으니 기존 입력을 얼마나 바꿀지를 학습한다. - 장점\n이 구조 덕분에 Gradient가 더 좋아진다. -\u0026gt; 역전파가 잘 전달된다! Bias도 더 좋아진다!! -\u0026gt; 학습 초기에 입력 그대로 전달하는 함수(identity)에 가까운 편향을 가지게 된다. 모델이 처음엔 뭔가를 바꾸지 않고 그냥 흘려보내는 것에 익숙해짐! [##Image|kage@ct3AVM/btsOMqhCltF/AAAAAAAAAAAAAAAAAAAAABDqhFU8a9GPRKkFriDpNB_hUlxvl6uchzwO2y-SB-ul/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=7B7VhBO4ErmPDWcu%2BWVCASiN2FA%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:413,\u0026ldquo;originHeight\u0026rdquo;:275,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n3. Layer normalization Layer normalizaiton이 model이 더 빠르게 train되도록 도와주는 trick이다. LayerNorm은 단어 벡터 내부의 값 분포를 정규화해서, 학습을 안정화하고 빠르게 만드는 기법입니다.\nIdea 각 layer에서 hidden vector의 불필요한, 의미없는 variation(잡음)을 cut down하기 위해, standard deviation(표준 편차)과 unit mean을 normalizing하는 것! (평균이 0이고 표준편차가 1이 되도록 정규화하는 것!)\n이게 잘 작동하는 이유 중 하나는, gradient(기울기)를 안정적으로 정규화해주기 때문이다.\n𝑥 ∈ℝ𝑑 : model에서의 한 individual(word) vector , d차원(embedding 된 차원)임\n𝜇 =σ𝑗=1 𝑑 𝑥𝑗 : 이건 mean이에여 𝜇 ∈ ℝ, x의 각 성분을 평균 내면 scala 값이 된다. 𝜎 2 = 1 𝑑 𝑑 σ 𝑗 = 1 𝑥 𝑗 − 𝜇 ; 이건 standard deviation이에여 , 𝜎 ∈ ℝ. 평균에서 얼마나 퍼져 있는지를 측정하면 분산 혹은 표준편차가 된다. 𝛾 ∈ℝ𝑑 : gain parameters 학습 가능한 parameter\n𝛽 ∈ℝ𝑑 :bias parameters 학습 가능한 parameter\n위의 두 parameter는 최종 출력 scale을 조정하는데 사용, 필요없으면 생략 가능함.\n또한 정규화 해주다가 정보 손실의 문제가 있을 수 있으니 복원 가능성을 주기 위해 학습 가능한 파라미터를 붙인다. 그럼녀 layer normalization은 다음과 같이 compute 된다. [##Image|kage@bzundi/btsOLdwZG81/AAAAAAAAAAAAAAAAAAAAAPIoF1Mu9BEVX6I6BDGP-DfyqObOIQZkGKQMH-QLG_B/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=hDiFjO8Rby2Z758WU2LPXSb7Fo4%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:860,\u0026ldquo;originHeight\u0026rdquo;:128,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}_##]\n입력 vecotr에서 평균을 빼고 표준편차로 나워서 정규화 한 후 감마와 베타로 scale 조정 짜잔! Transforemer Decoder는 다음과 같은 Block으로 이루어졌답니다. [##Image|kage@7J3Ex/btsOK9nKDyq/AAAAAAAAAAAAAAAAAAAAAGIfAhhGOKFkXFLsXkKguFv5zwYiFQ-E7AOOotqaA9aT/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=jKPRgP%2FzTDkKagUu3Iu2FyXG0CE%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:386,\u0026ldquo;originHeight\u0026rdquo;:662,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nSelf-attention Add \u0026amp; Norm (이건 각 단계마다 붙여주는 것이라 생각하면 된다) Feed-Forward Add \u0026amp; Norm 궁금한 것!! \u0026ldquo;n × d에서 n은 height고 d는 width처럼 봤는데,\nreshape해서 n × h × d/h가 되면 n이 channel이 되는 거야? 아니면 여전히 height야?\u0026rdquo;\n❗️n은 여전히 height (sequence axis), 즉 \u0026lsquo;단어 수\u0026rsquo;를 나타내는 축이야.\n→ channel 축은 h × d/h 쪽임 (즉, 나중 차원들)\n🔸 왜 그렇게 해석해야 하는가? 1. 원래 텐서 구조: XQ∈Rn×dXQ ∈ ℝ^{n × d}XQ∈Rn×d\nn: 단어 개수 → 문장 길이 → height (세로) d: 각 단어의 임베딩 → width (가로) 이 구조는 마치 이미지에서:\narduino\n복사편집\nheight x width\n처럼 보임. (2D)\n2. reshape: Rn×h×d/hℝ^{n × h × d/h}Rn×h×d/h\nn은 그대로: 단어 수 → 그대로 height (세로) h × d/h는: 벡터를 head 수로 분리한 것 → 이게 channel 방향 확장처럼 작동 🔸 이미지와 비교해서 대응시켜보면: 자연어 처리 텐서이미지 처리 텐서의미\nn height 단어 위치 d (or h × d/h) channel 임베딩 차원 / attention head (없음) width (단어는 1D이므로 가로축 없음) ✅ 요점 다시 말하면: n × d: n은 height처럼 \u0026ldquo;단어 위치\u0026rdquo;, d는 임베딩 (channel처럼 해석 가능) n × h × d/h: n은 여전히 height (단어 위치),\n→ h × d/h는 channel dimension처럼 작동 (각 단어당 여러 head 표현) ❗️channel은 맨 뒤쪽 차원이고, height는 항상 단어 순서(n)\nreshape를 해도 n이 channel이 되는 건 절대 아님\n🔸 감각 비유 이미지에서는:\nless\n복사편집\n이미지: \\[Height × Width × Channel\\] 문장 입력: \\[Token 위치 × Feature\\] → reshape → \\[Token 위치 × Head × Dim\\_per\\_Head\\]그래서 문장에서 Token 위치(n) 는 항상 height 역할이지 channel이 아니야.\n✅ 요약 정리 reshape 전reshape 후n의 의미channel은 어디?\nn × d n × h × d/h ✅ 여전히 단어 위치 (height) ✅ h × d/h (뒤쪽이 channel 역할) channel인가? ❌ 아님 ❌ 궁금한 것!!222 matrix의 의미가 고정되어 있는 게 아니라\u0026hellip; vision인지, LLM인지 다 다른 거야?\u0026rdquo;\n✅ 맞아. 텐서(행렬)의 각 차원이 뭘 의미하는지는 \u0026lsquo;도메인\u0026rsquo;에 따라 달라.\n예시: 도메인별 같은 차원이 의미가 다름 도메인텐서 차원차원 의미\nVision (CNN) \\[B, C, H, W\\] B: 배치, C: 채널, H: 높이, W: 너비 LLM (Transformer NLP) \\[B, N, D\\] B: 배치, N: 토큰 수, D: 임베딩 차원 Multi-Head Attention \\[B, H, N, D/H\\] B: 배치, H: head 수, N: 단어 수, D/H: head당 임베딩 Audio \\[B, T, F\\] B: 배치, T: 시간축, F: 주파수 bin Time series \\[B, T, C\\] B: 배치, T: 타임스텝, C: 변수 (채널) ","date":"2025-06-22T00:34:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/transformer-decoder/","title":"Transformer - Decoder"},{"content":"우리가 앞에서 Attention이 input sequence로부터 입력 ht에 중요한 information을 넘겨주는 것이라고 배웠다! 근데 RNN도 과거 시점의 information을 input로 전달하는 같은 목적이 다! 그렇다면\u0026hellip; RNN을 굳이 쓸 필요가 있을까?? Attention이 정보를 넘겨주는 방식이라면! 그래서 Transformer가 등장!\nSelf-attention Transformer를 이해하기 위해서는 self-attention을 이해해야 한다. Cross Attention은\ngenerate yt를 생성하기 위해 input x에 attention하 것! (지난 시간 배운 attention)\n여기서 Self-Attention은\nyt를 생성하기 위해 y\u0026lt;t에 attention하는 것!\n즉, output seq를 생성하는데 현재 시점 t의 출력을 생성할 때 이전 출력들을 보고 생성한다! 이전 토큰들을 참고해서 중요도를 따지고 이용해서 생성한다! [##Image|kage@be8zpa/btsOLD9Pu2C/AAAAAAAAAAAAAAAAAAAAABekskayoAGc-4KwI0taA2S2n5x75RR84yoJp3LmHM5/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=oPXBMgLREoUk%2Bs1lXcOvkMDidx0%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:694,\u0026ldquo;originHeight\u0026rdquo;:431,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}_##]\nKeys, Queries, Values 자 vocabulary V의 단어 sequence w1:n이 있다고 하자 (e.g Zuko made his uncle tea)\n1. Embedding matrix E에 대하여 word embedding으로 바꾼다. 각 wi -\u0026gt; xi 로 바꿔준다. (Xi = Ewi)\n2. weight Matrix Q, K, V를 각 word embedding으로 바꾼다. [##Image|kage@3bxeM/btsOLCJObQm/AAAAAAAAAAAAAAAAAAAAAPPwJYTdEqOoY-1m_a_89pdr78vQeiFFObvbF_PwsQSB/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=knLrb8DGb9z5D0QLpUUaxb5jFoM%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:704,\u0026ldquo;originHeight\u0026rdquo;:50,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n3. Key와 Query의 similiaites를 계산하고 softmax로 normalize한다! [##Image|kage@pu8xq/btsOLhTA3Ic/AAAAAAAAAAAAAAAAAAAAAEdv3PkowQlo4wCjxNURej_xaWUEUbOeP8IDwZKwTaeX/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=EzMQQfvZ6qdz2wZnvtl1PM23nNk%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:440,\u0026ldquo;originHeight\u0026rdquo;:95,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n4. 그것을 가중합하여 계산!~~~\n[##Image|kage@brX2hH/btsOMOCt1Ss/AAAAAAAAAAAAAAAAAAAAAFHwzM5XzmrME2N66kQYaip9Xke9JpxQYgjLwixyGUxc/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=kFWt2skh%2Bd33OGIOUrXlvJKV6B4%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:170,\u0026ldquo;originHeight\u0026rdquo;:89,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nSelf Attention의 장벽.. 그리고 Solutions! 1. 단어의 순서에 대한 정보가 아예 없잖아! : Sequence order [##Image|kage@ZtbFh/btsOLwJAES9/AAAAAAAAAAAAAAAAAAAAADKFLj7QwjL3cOCIukImqTwR25LakhzG6y-TD18VxGnq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=eJLlud8Oj3YYy2srFT%2F%2F%2BpJp37M%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:473,\u0026ldquo;originHeight\u0026rdquo;:65,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nSollutions: Position representations을 input에 추가해준다!\n즉, key, query, value를 만들기 이전에 input embedding에 order information을 미리 넣어줘야 한다!\npoisional encoding인 vector Pi 는 pi가 문장의 특정 위치(index)에 있음을 가리킨다. (실제로 sin,cos 로 구성된 고정된 vector 값임)\n[##Image|kage@VkgXq/btsOMP2sD9u/AAAAAAAAAAAAAAAAAAAAAAasu_r12kGGo8RBan-Ib1umxwTvS6JhqtmjjiuDG94M/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=BeQP0E3V%2B7bN4r5dIH3Y33d8z6U%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:529,\u0026ldquo;originHeight\u0026rdquo;:158,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n이를 input embedding에 그냥 더해주면 된다~~ 그래서 xi~는 위치 정보를 반영한 최종 embedding이므로 positioned embedding, positional input이라고 부른다. 어떻게 Position representation을 만들까? 1. Sinusoidal position representations\n[##Image|kage@c9Vr33/btsOLhzgeUc/AAAAAAAAAAAAAAAAAAAAAJavoxqGN70ZHvmnz\u0026ndash;tohf_l3ngojWt1gVCfmWjyhdx/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=bKmPA5WExLRHkchrf7xjRJVyxkI%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:864,\u0026ldquo;originHeight\u0026rdquo;:209,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nsin, cos 함수로 변환해서 벡터로 표현! 장점\n- vector 차원마다 주기와 dimension이 다르기 때문에 다양한 시점의 패턴을 표현 간으! - 완전한 절대적 위치가 아닌 상대적 위치 차이(간격)이 더 중요하게 작동함! - 입력 sequence가 길어져도 패턴이 반복될 수 있다! 그래서 안 본 길이까지 일반화(extrapolation)할 수 있음! 단점\n- Not learnable: 완전히 고정된 함수로 만들어져서, 학습 중에 모델이 자유롭게 조정하거나 최적화 X\n- extrapolation 사실 안됨 : 긴 입력에 대해서 성능이 똑같이 잘 안 나옴\u0026hellip; 2. scratch(처음부터, 제로로)로 부터 Position representation vectors를 학습시키자!\nAbsolute poistion representation을 배우자!\npi를 학습가능한 parameter도 두자~~ pi는 P matrix의 matrix column으로 두자! 장점\n- 유연하다. 단점\n- 일반화 X : 만약 훈련 시 최대 n의 길이인 문장 보았는데\u0026hellip; 그 이상의 문장이 들어오면?? 안 됨\n3. RoPE : Common, modern position embeddings\n다시 생각해보자. positioned embedding을 f(x, i) 형태로 relative position embedding으로서 생각해보자. f(x, i) : 단어 x의 위치 i에서의 embedding이라고 해보자 (x는 단어, i는 위치)\n[##Image|kage@bmsHwM/btsOMo48Apu/AAAAAAAAAAAAAAAAAAAAABCTMEWbF7JEplWdLSExxE0LJ4etKHholRd305xf5yI2/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=2tUd%2F%2FGuyVAjpc2GWv3hhXMOQY0%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:346,\u0026ldquo;originHeight\u0026rdquo;:79,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n여기서 Attention function g는 x, y의 의미와 (i-j)라는 상대적 위치 차이에만 의존해야 한다!!\n기존의 positional embedding은 이 목표를 충족시키지 못 하는가? - 1의 경우 : 상대적이지 않은 다양한 cross-term을 가진다. (절대 위치 정보가 따로따로 반영된 것) - 2의 경우 : (i-j)에 대하여 inner product로 표현되지 않는다.\nEmbedding via rotation 우리는 position embedding이 absolute position에 영향을 받지 않게 하고 싶음! -\u0026gt; relative position으로! 우리는 inner products가 arbitrary rotation에 변하지 않는다는 것을 알고 있어! -\u0026gt; 내적 값은 동일해! [##Image|kage@DtFFE/btsOLSr9vWn/AAAAAAAAAAAAAAAAAAAAADePNAOx6AlQof92jWFhowuivem3neJo5rh3uBiwXpN8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=Q0rZ6D1%2F5q6iS2VU%2Fu6AMsCUFts%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:829,\u0026ldquo;originHeight\u0026rdquo;:306,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:677,\u0026ldquo;height\u0026rdquo;:250}##]\n내적 기반으로 attention score를 계산하면..! 상대 위치 기반 attention이 가능하다! [##Image|kage@1fMUn/btsOKA7fddu/AAAAAAAAAAAAAAAAAAAAAFiY4bPdxRyyNhxboOvoLfxVa_NpvRX9zY2gSX9qV884/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=0Ypa%2B5YtdisnSDC%2BdvW7G6TNGCQ%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:766,\u0026ldquo;originHeight\u0026rdquo;:439,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n따라서 coordinate를 짝지어서 2D 평면에서 회전시켜라!!! (아이디어는 복소수에서 따온 것!!) 즉 위치 i에 대하여 단어 vector x에 대해서 sin/cos을 곱하여 벡터 공간에서 회전시키자! 위의 그림을 보면 각 단어는 d 차원의 Query/Key 벡터를 가진다! 그리고 poision이 적용 된 후 Position embdding(위치 정보가 내제된) Query/Key가 생성된다. 단어 Enhanced에 대한 Query와 Key vector (x1, x2)가 존재하고 position m(index임)이 있다.\n이때 Q, K vector(x1, x2)를 sin/cos 기반으로 m만큼 회전시킨다. 그러면 반영이 되고!! 그러면!! 회전이 반영된 vector된 후 Enhanced라는 Q 벡터(위치 1)와 Rotary라는 단어의 K 벡터(위치 4)가 dot product하여 attention score 계산 시 위치 정보가 반영이 된다~~~ 2. Deep Larning을 위한 nonlinearities가 없워!! 그냥 단순히 weighted 평균이자냐 : non-linearities [##Image|kage@cnQ8RS/btsOLBKXrNm/AAAAAAAAAAAAAAAAAAAAAFKU9q_Lzhwt1sEmCWN69mWDpMJBFc3f2Bne5KxbOL6F/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=PH05iw82z83oI%2Bz%2FWBPsUqLeBgQ%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:684,\u0026ldquo;originHeight\u0026rdquo;:530,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:506,\u0026ldquo;height\u0026rdquo;:392}##]\nSelf-attention 메커니즘 자체에는 비선형성 이 없다\u0026hellip; 전부 선형 연산임!!\n따라서 Self-attention만 계속 쌓다보면\n이전 layer에서 얻은 Value vector들을 다른 방식으로 재평균(re-averaging) 하는 것에 불과하게 됨 Solutions: Self-attention 이후 Feed-Forward network로 비선형성을 추가하여 각 output vector를 처리함! [##Image|kage@bC9JFW/btsOMaTAZXU/AAAAAAAAAAAAAAAAAAAAAIMhHk4CCtIMmh3ZLczQgxLDfGQkZneu9oxm_6RbHjjj/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=ZwTxvW9sZnvB1u21ip%2FiGgjZ8wk%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:492,\u0026ldquo;originHeight\u0026rdquo;:141,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n3. sequence를 예측할 때 \u0026ldquo;미래를 보지 않는다\u0026quot;는 보장이 있나? : Masking the future in self-attnetion 이 말은 즉슨, Machine translation이나 Language modeling과 같이 순차적인 task에서 아직 나오지 않은 단어를 미리 참고하면 안 된다는 뜻!!! (self-attention은 input 전체를 확인하니까!) -\u0026gt; 즉, training 할 때! (inference는 아님!!) [##Image|kage@ddgVDd/btsOKS0PxOh/AAAAAAAAAAAAAAAAAAAAAOi-yB3vie78rxzMKs93S6BqZjHFUgrvqUUa1bHpPxsR/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=cyvxv8gcPjAo1aOSFR5x51ViWMo%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:704,\u0026ldquo;originHeight\u0026rdquo;:298,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:324,\u0026ldquo;height\u0026rdquo;:137}##]\nDecoder는 기본적으로 self-attention과 cross-attention을 사용한는데,\ntrain 도중에 input과 target을 모두 받는다. 이때 단어 예측하는데 self-attention 시 target seq 전체를 참고할 수 있으니, 이를 가려야 함.\n그러면 매 timestep마다 keys들과 querys를 바꿔서 이전 단어만 보게 만들면 될까? -\u0026gt; Inefficient!\n매 timestep마다 attention 범위를 다시 설정해야 하므로 병렬 계산이 불가능\u0026hellip; 즉, paralleization이 가능하려면\n- 전체 seq를 다 넣은 후\n- future 단어들의 attention scores를 -무한대로 설정하여 attention을 mask out 처리해야 함! - softmax에서 exp (−∞ ) = 0이므로 완전히 무시된다~~\n[##Image|kage@TRTZW/btsOLhF2Z4s/AAAAAAAAAAAAAAAAAAAAADuKmel_ldl2RlpnDJcyRSNAbSZzMBz7mppbBHytCNAl/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=9kk6y0Xu1gbYNWN3q28meDty3dY%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:240,\u0026ldquo;originHeight\u0026rdquo;:93,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##][##Image|kage@baAlZ0/btsONayxgTE/AAAAAAAAAAAAAAAAAAAAANjvvtTqGeILUDd_ZK6OgOMF1lUF9iu1TwhfAOM-A1J5/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=bow2jZu5lSfPkPtaH5X1Y6%2F0soM%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:729,\u0026ldquo;originHeight\u0026rdquo;:587,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:507,\u0026ldquo;height\u0026rdquo;:408}##]\nSolutions: attention weight를 0으로 일부러 만들어서 미래를 mask out한다. 정리 : Self-attention을 위한 building block! [##Image|kage@brSmTy/btsOKUdjH2z/AAAAAAAAAAAAAAAAAAAAAMg4S7fYAb3shTEAmgIlyP-GYvVUNOoPsSiBoIg6-Gct/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=oG%2FancrmYQbbA1RrMiYyw7MZXLc%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:405,\u0026ldquo;originHeight\u0026rdquo;:605,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n1. self-attention 2. Position representations\nseq oreder를 구체화하기 위해서! self-attention은 input의 순서를 고려하지 않기 때문에! 3. nonlineariies\nself-attention block의 output에 Feed Forwar newrok를 구현!\n4. Masking\n미래를 보지 않고 parallize 연산을 하기 위해서 미래에 생성돼야 할 단어의 정보가 앞 단어 예측 과정에 흘러 들어가면 안 됨 ❌ Masking은 이런 **정보 누출(leakage)**을 막아줌 ","date":"2025-06-21T22:36:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/transformer1-self-attention/","title":"Transformer(1) - Self-attention"},{"content":"이전에 RNN의 문제에 대해서 알아보았다.. - 병렬 처리 안 됨 - 선형 구조에 의존되어 문장을 이해하기 때문에 잘 처리하지 못함! 위와 같은 문제를 해결하기 위해 나온 solution이 Attention이다. Core Idea decoder의 각 step에서 encoder와의 직접 연결을 이용해서, source sequence의 특정 part에 집중한다.\n전통적인 RNN은 인코더의 마지막 hidden state 하나만 보고 다음 단어를 예측했다! (정보 손실이 크다!! )\nBasic Idea attention이 없을 때 다음의 아이디어를 떠올렸다\u0026hellip;\n그러면\u0026hellip; hidden state 정보 하나에 이전의 정보가 모두 잘 요약했다고 보장할 수 없으니까\u0026hellip;? 그걸 다 요약하는 거 어때??\n즉,\nEncoder로부터 정보를 전달하는 가장 기본적인 방법은\u0026hellip; encoder의 hidden state들을 평균(average) 내는 것이다!! 즉, decoder에 Encoder의 context vector를 전달하는 것! [##Image|kage@DzINe/btsOL8VNoTS/AAAAAAAAAAAAAAAAAAAAADSpoVkTOAsA7wF9MjZMkmiyPmzuPpUAJixANG3RSohv/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=NXR3m48WTG8XNRTJBf0gAnQSJ3Y%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:513,\u0026ldquo;originHeight\u0026rdquo;:331,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n문장의 encoding을 계산할 떄 모든 hidden state들의 element-wise max 또는 mean을 취하는 것! Attention 즉 weighted average를 통해서\u0026hellip; weight가 높은 것을 비중으로 학습하는거지!!! [##Image|kage@bIAhbq/btsOL3ty6o4/AAAAAAAAAAAAAAAAAAAAACs-ThR3rgXTYPTssuVKhnURBuKVs2n-EG99t9BcnWl/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=Nw566sYGRUABPHE7sXEyNVrjP2U%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:446,\u0026ldquo;originHeight\u0026rdquo;:299,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}_##]\nAttention에는 Query, Keys, Values로 구성되어 있다. Query는 모든 Keys들과 부드럽게 매칭되어 0과 1 사이의 weight를 만든다. 그 weight를 Key에 해당하는 Value를 곱하고 모두 더한다. [##Image|kage@r2Fiy/btsOLFT6U3Y/AAAAAAAAAAAAAAAAAAAAADVSA91a0Adca3w9eUKGdt8oU8SvCWTfee6_W5xrSTp0/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=M1U58ZHRsAVKoxonVNAMmI9N%2ByI%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:344,\u0026ldquo;originHeight\u0026rdquo;:288,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nLookup table은 key들을 value에 Mapping한 table이다. Query가 특정 Key와 일치하면, 그 Key에 해당하는 Value를 반환한다. seq2seq with attention의 작동 방식 [##Image|kage@l4yAK/btsOLEgvH74/AAAAAAAAAAAAAAAAAAAAAPdoukWGC6LQxaiMEPsCv4MI-r_AcVKtzSjoi0XZLDGq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=HKYtM0yCNoBE4S66vj2GEuJPpEE%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:746,\u0026ldquo;originHeight\u0026rdquo;:407,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n빨간색은 Encoder, 초록색은 Decoder 부분이라고 하자! 1. 각 input의 hidden state에 대해서 현재 st(decoder의 초기화 vector)를 곱해서 Attention scores를 계산한다. [##Image|kage@mOuKB/btsOKqQYw8p/AAAAAAAAAAAAAAAAAAAAAGPQ6q_U9671wVdFeZTwEgEA17_CnmsLWW11L6pPpGxS/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=p0B%2BCxP3mBSdvZy36bJcegFs%2FhA%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:593,\u0026ldquo;originHeight\u0026rdquo;:459,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n2. SoftMax를 이용해서 계산한 scores를 probability distribution(이것이 진짜 가중치!)으로 바꿔준다. Decoder timestep에서는 우리는 제일 먼저 encdoer hidden state 를 먼저 focusing 한다는 것을 알 수 있다. [##Image|kage@bITsJ5/btsOMGRY8EJ/AAAAAAAAAAAAAAAAAAAAAL7KQOnnV7YuFMhrG-J9nAEXHgTGVkMlCT4YyZ4wKEsI/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=mB%2BI%2Fdz6tTmEBS3MlJA%2BryCYPrU%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:742,\u0026ldquo;originHeight\u0026rdquo;:479,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:496,\u0026ldquo;height\u0026rdquo;:320}##]\n3. 이 attention distribution(가중치)을 이용해서 encoder hidden state의 가중합을 계산한다. 계산된 attention output은 대부분 high attention을 가진 hidden state의 정보를 포함하게 된다. [##Image|kage@8IDHU/btsOKQomlFc/AAAAAAAAAAAAAAAAAAAAAFTLQ5a8Hx_aJPumAAmX_MDdOpwmrwrXYQDGH8c5z0FW/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=JUbcSl3rBUdudzevAyWIS3496fU%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:736,\u0026ldquo;originHeight\u0026rdquo;:457,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:581,\u0026ldquo;height\u0026rdquo;:361}##]\n6. attention output (context output)과 decoder hidden state를 연결해서 이를 기반으로 y1을 계산한다. [##Image|kage@beZtrw/btsOMPg4qZg/AAAAAAAAAAAAAAAAAAAAAOh0qj7u_Q6kTeCNhG8flJLw-JIeY8Duad0puVbSMqg7/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=k8xTX3QjOZJWZkTfDEckIaq8uXQ%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:748,\u0026ldquo;originHeight\u0026rdquo;:467,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:529,\u0026ldquo;height\u0026rdquo;:330}##]\n어떤 경우에는 이전 step의 attention output도 decoder input에 같이 넣는다. In equations : 수식으로! 1. Encoder의 hidden states가 주어지고, timestep t에 대해서 decoder hidden state st가 있다고 하자. [##Image|kage@qmnw5/btsOLheU80P/AAAAAAAAAAAAAAAAAAAAABqmL3BeKs1-WwcucDIqZNZNwWTw4RZlWTUCuCAe2i6x/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=YCjEosVPZzeYKkXIP0SesBtVBLk%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:149,\u0026ldquo;originHeight\u0026rdquo;:26,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##][##Image|kage@vdWwJ/btsOMasxrYh/AAAAAAAAAAAAAAAAAAAAAD5-9-XL4cOrEGwkLVzDy1XnV4vVSkZ917stNgHgXpTl/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=sTgDFYKKxqQtemRVdUG5sJJx4CA%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:72,\u0026ldquo;originHeight\u0026rdquo;:27,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n2. 이 step에 대해서 attention scores(단시 유사도 점수!) 를 계산하자! [##Image|kage@bqTZ13/btsOKIcOqUM/AAAAAAAAAAAAAAAAAAAAAOzDKAMvg_w2UBQGSoDcRJ4CtMkT_RwxyMy-PXABjB1t/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=YjzML%2FZ8lXjTYNfgV49kEqPRS5w%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:319,\u0026ldquo;originHeight\u0026rdquo;:54,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n즉 현재 st(Query)와 입력의 hidden state(\nValues\n)를 곱하면 된다. 3. 이 step의 attention distribution(attention Weight)을 계산한다. softmax 함수로! (확률 분포를 계산해주고, 합은 1)\n즉, 각 st가 encoder hidden state에 대해서 얼마나 집중할지 확률처럼 해석 가능!! [##Image|kage@bEvDUD/btsOKAsxBCz/AAAAAAAAAAAAAAAAAAAAAN3SAvBNcgPrYThoCliDXHKIj6l-4fUpJYJYpwck8bDS/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=I3MdRz9DP3w8zkXggsLSnnuWm2c%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:264,\u0026ldquo;originHeight\u0026rdquo;:52,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n4. 이 attention distribution(attention Weight)을 encoder hidden state(Values)와 weighted sum하여 attention output를 얻는다. [##Image|kage@biOpXj/btsOMPaiSA6/AAAAAAAAAAAAAAAAAAAAAFZ8T5xInYjJeBn7dAg5MluLo2fCkUByTSaICLoT7Jri/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=o15kVaoMIH3jFZ72I8ynHHSJREE%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:192,\u0026ldquo;originHeight\u0026rdquo;:73,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n5. 이 attention output at와 decoder hidden state st를 이어 붙이고 decoder로 넘겨준다!! 정리..! Query qtq_t 디코더의 현재 hidden state (t 시점) “지금 어떤 정보가 필요하지?” Keys kik_i 인코더의 각 hidden state 입력 단어들의 \u0026ldquo;정체성\u0026rdquo; Values viv_i 보통 Keys와 같고, 인코더의 hidden state 자체 입력 단어들의 실제 정보 Attention distribution Query와 각 Key 간의 유사도 → softmax로 만든 가중치들 α1,α2,…,αT\\alpha_1, \\alpha_2, \\dots, \\alpha_T, 총합 = 1 Attention output (context vector) ∑iαi⋅vi\\sum_i \\alpha_i \\cdot v_i 집중한 값들의 가중 평균 Attention is awesome! Attention은 parallelizable하고 bottlenck issues를 해결할 수 있다. 즉, 각 단어의 representation(==vector)을 Query로 사용해서 Values에서 정보를 가져오고 결합한다. == 각 단어(Query)는 모든 단어를 탐색하여(key) 자기에게 필요한 정보만 골라낸다! 우리가 지금까지 본 것은 Cross-Attention(Decoder가 Encoder를 쳐다보는 것) 이제는 한 문장 안에서 단어들끼리 서로 주목하는 Self-Attention (단어들끼리 서로를 동시에 쳐다봄)을 보자!! Self-attention은 특히 순차적으로 처리해야 하는 연산의 개수가 sequence 길이에 따라 늘어나지 않는다는 장점이 있다! 즉 GPU에 친화적이다!!\n모든 word가 한 layer에서 서로 상호작용 하기 때문에 최대 interaction distance는 O(1)이다. [##Image|kage@bJvhuO/btsOLqQdNqO/AAAAAAAAAAAAAAAAAAAAAPw0Kk8Tq9MJ_Y4-LHuDkA61C7WrHa04922uXhjVYdto/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=dryUYJGjATLAKGTSRH4XsjvRICw%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:460,\u0026ldquo;originHeight\u0026rdquo;:209,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nAttention is awesome Deep Learning technique 우리는\u0026hellip; seq2seq 모델을 Machine Translation 하기 위해 개선한 방법인 attention을 보았습니다..\n하지만!! 다른 많은 architextures, 그리고 많은 task에서 사용된다다!! More general definition of attention : • Given a set of vector values, and a vector query, attention a weighted sum of the values, dependent on the query.\n어떤 경우에는 query 가 value에 attend한다. 라고 말한다. 지금까지 우리가 본 seq2seq + attention 구조에서는 Decoder의 각 hidden state (query)가 Encoder의 모든 hidden state (value)에 attention한다. (즉, 어떤 정보가 필요한지 attention을 통해 결정!)\n정리: 직관 가중합 : query가 집중해야 할 값을 결정한 values들이 포함된 정보의 선택적 요약, Attention은 아무리 많은 representation이 있어도 하나의 고정된 fixed-size representation(vector)을 뽑아낸다. 그 vector는 query가 어디에 집중했는지에 따라 달라진다. Attention은 거의 모든 Deep Learning model에서 가장 강력하고 유연하고 일반적인 pointer이자 memory manipulation이다. 그래서 이 아이디어가 적용된게 NMT여! 추가 궁금증\u0026hellip; 왜 Query(st)랑 Keys(input hidden state)를 dot product하는게 정보를 결합하는게 되는거야? dot product는 결국 두 입력이 얼마나 관련 있는지 (유사도)를 측정하는 것! 그래서 Query(st)랑 Key(입력)얼마나 관련이 있는지 측정하고 value(=정보)를 얼마나 가져올지 결정하는게 정보 결합!!!!\n근데 위의 모델에서는\nKey = Value = Encoder Hidden State로 그냥 써버린 것!! Transformer에서는 Key, Value는 hidden state에 다른 가중치 marix를 곱한다.\n","date":"2025-06-21T20:02:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/attention/","title":"Attention"},{"content":"sentence x를 source language에서 target lanuge인 sentence y로 번역하는 task\n[##Image|kage@pGqje/btsOLcLteEK/AAAAAAAAAAAAAAAAAAAAAJ8N5fEEcNKATTcOrT6v8Z2T9AlfGSSn9AL_69fY9USs/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=eajn9PkUyAUTu8iHDsP0Rqep3SQ%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:575,\u0026ldquo;originHeight\u0026rdquo;:248,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nStatistical Machine Translation Core Idea\ndata로부터 probabilistic model을 학습하자!!\n[##Image|kage@bshULM/btsOL9UEEqN/AAAAAAAAAAAAAAAAAAAAAIp1l1JOWvn-gSVfpn_N1HdbHlVlG2rHxsPH8wbZkC2g/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=rmHlRNvtGTV4s%2B%2BZPCEf9yo8ZuM%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:710,\u0026ldquo;originHeight\u0026rdquo;:380,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n프랑스어 sentence x가 주어졌을 때, sentence y를 찾아보자!!!\n즉 2개의 component로 각각 분리해서 학습시킨다! Translation Modelword나 phrases를 어떻게 번역해야 하는지 모델링, parallel data로부터 학습됨 Language Model\n좋은 English를 어떻게 써야 하는지 모델링, monologual data로부터 학습됨 그러나\u0026hellip; 극도로 복잡하고 중요한 detail들이 너무 많다\u0026hellip; 갑자기\u0026hellip;등장한 것이 있으니..\nNeural Machine Translation (NMT) 구글 번역도 원래 SMT system이었는데..! NMT system으로 바뀌면서 빠르게 발전됨!!\n[##Image|kage@KztoA/btsOLjKAPJO/AAAAAAAAAAAAAAAAAAAAAKSVFfP_V1F1CGPM8qs4CtkTkr9qolVRwRl448JVY1b2/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=LYQiQbO%2BxDXQc%2BSMzU2ImHKugGo%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:748,\u0026ldquo;originHeight\u0026rdquo;:436,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:640,\u0026ldquo;height\u0026rdquo;:373}##]\nsequence-to-sequence model 이다!!\n**Encoder RNN\n**Encoder RNN은 source sentence의 encoding을 생성\nEncoding 결과는 Decoder의 초기 hidden state로 제공 **Decoder RNN\n**encoding을 조건으로 하여 target sentence를 생성하는 Language Model 즉 Encoder는 input을 취하고 neural representaion을 생성, Decoder는 neural representation을 기반으로 output을 생성\n이는 seq2seq model 이라고 부른다. 많은 NLP task에서는 seq-to-seq로 표현될 수 있음!\nSummarization (long text → short text) Dialogue (previous utterances → next utterance) Parsing (input text → output parse as sequence) Code generation (natural language → Python code) NMT [##Image|kage@el1MRG/btsOLt0scSI/AAAAAAAAAAAAAAAAAAAAAPIidx9a-aWVJQotstRic7e-OxaafFnqpJygAm0-iGFH/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=aj7Nv2EZj29i%2B5XsCKiWclilJMs%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:654,\u0026ldquo;originHeight\u0026rdquo;:165,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nseq2seq 모델은 Conditional Laguage Model의 한 예시임!\nLanguage model인 이유는 decoder가 target sentece y의 다음 word를 예측하기 때문이고 Conditional인 이유는 predction이 이전 단어들에 더해 source sentece에 의존하기 때문! 어떻게 train? [##Image|kage@4Twsx/btsOMmM2cZG/AAAAAAAAAAAAAAAAAAAAAKAe9xc8Q4WOYWbkWdvIrtxE287Km-HlwKyzNev4TxfS/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=oNJVT4MAY3sM9mEKBHnEseFO0mY%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:774,\u0026ldquo;originHeight\u0026rdquo;:453,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n즉 loss는 decoder에서 계산되고\u0026hellip; 하나의 systemdㅡ로서 end-to-end로 Backpropagation함! 문제점: issues with recurrent models Linear interaction distance O(sequence length) 단계의 거리의 word pairs간 상호작용한다는 뜻은\u0026hellip;\n긴 거리의 dependencies은 학습하기 어렵다..(grdient 소실 문제 여전히\u0026hellip;)\n→ 예: \u0026ldquo;The dog that chased the cat ran\u0026rdquo; → \u0026ldquo;dog\u0026quot;과 \u0026ldquo;ran\u0026quot;은 멀리 떨어져 있음\nRNN은 그냥 왼쪽부터 오른쪽으로 하나씩 처리하기 때문에 문장 구조에 Linear order가 장제로 박혀있다.\n문장을 단순한 순서로만 생각해서는 안 된다..! \u0026ldquo;The dog that chased the cat ran away.\u0026rdquo;\n→ 주어는 \u0026ldquo;The dog\u0026rdquo;, 동사는 \u0026ldquo;ran\u0026rdquo;, 근데 둘 사이에 관계 있는 단어들이 멀리 떨어져 있음\n즉 이를 순서대로 처리하는 모델은 구문 구조나 의미 관계를 잘 파악하지 못한다. Lack of parallelizability Unparallelizable하게 작동하기 때문에 forward와 backward 연산은 O(seq length)를 가진다. GPU 병렬 처리에 잘 안 맞고..(이전 hidden state를 계산해야 다음 hidden state를 계산할 수 있기 때문!!) 큰 데이터 학습이 힘들다!!\n","date":"2025-06-21T18:13:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/machine-translation-mt%EC%9D%98-%EB%B0%9C%EC%A0%84/","title":"Machine Translation (MT)의 발전"},{"content":"vanishing gradients의 문제를 해결하고자 제안된 RNN의 한 종류임!\n순차적으로 처리!\nget, cell 등 내부 구조\n","date":"2025-06-21T17:32:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/long-short-term-memory-lstm/","title":"Long Short-Term Memory (LSTM)"},{"content":"Core Idea [##Image|kage@cd3kQx/btsOL55YG2J/AAAAAAAAAAAAAAAAAAAAANYIGed86TNJRULXuEeGhsPDL1fSZKTEgFCyVcJT_nnM/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=xtXMYU30gOU9Dw%2FwaYztfGNHw2E%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:620,\u0026ldquo;originHeight\u0026rdquo;:302,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n같은 가중치 W를 반복해서 적용한다!!!!\n각 입력에 대해서 hidden state에 같은 W를 곱한것을 같이 처리하는 것을 볼 수 있다. (이전에는 x1은 w1, x2는 w2랑 곱해져왔음) RNN의 전체 구조 [##Image|kage@nAlnd/btsOKCKGYh2/AAAAAAAAAAAAAAAAAAAAAI_fpqbVrSvAnP94hHCY_7jcCxy1K1hw80WqHlf7PRzI/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=agopE1K1jweMPlppzIrc%2BVSfsUY%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:761,\u0026ldquo;originHeight\u0026rdquo;:525,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n장점 어떤 길이의 input이든 처리 가능 step t에 대한 계산은 많은 뒤의 step의 정보를 활용한다.(이전 정보를 활용한다!) input 크기에 따라 Model size(Wh, We)가 증가하지 않는다. 모든 시간 step t에 같은 W를 적용하기 때문에 위치에 상관없이 공정하게 처리된다. 단점 Recurrent computation이 너무 느려 -\u0026gt; 병렬 처리가 안 된다!! 완전 이전 step의 정보를 얻는게 좀 어려움\u0026hellip; -\u0026gt; 정보 소실 문제! (gradient vanishing) Training Input : 단어들의 sequence Ouput : 모든 각 step t의 distribution yt Loss : step t에 대해서, 예측 확률 분포 yt와 진짜 다음 단어 yt 간의 cross-entoropy(불일치를 측정하는 함수)를 계산 그 Loss를 모두 더해서 평균을 낸다. * cross-entropy?\n예측한 확률 분포yt와 실제 정답(label)의 분포 yt 사이의 불일치를 측정, 모델에 확신을 가지고 맞췄는지를 평가하는 Loss 함수! 따라서 정답을 맞춰도 확신이 없으면 Loss가 커진다\u0026hellip; [##Image|kage@bu6ZvL/btsOL8OXDOx/AAAAAAAAAAAAAAAAAAAAANYGFDr1VnpHth2STLiZISwfe_wqt8My-valyxnwKa7Y/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=OBFojuUdiS4Ro%2FJuzjm6fOXkAGA%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:616,\u0026ldquo;originHeight\u0026rdquo;:255,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##][##Image|kage@qz5G4/btsOK40UBYS/AAAAAAAAAAAAAAAAAAAAAPgHM10-ONI6fduzzG8qC55ZS0yeKIB5r3a_e-eiFBq3/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=Ln447zxoSPmQ9%2B6wXoRP9D%2Fvt%2BM%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:839,\u0026ldquo;originHeight\u0026rdquo;:522,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:681,\u0026ldquo;height\u0026rdquo;:424}##]\n1. x1이 들어와서 y1을 예측\n2. 이때 Loss는 실제 true값(y2) 와의 nevative log prob을 계산한다. 3. time step t마다 계산 후에 4. Loss를 계산 후 평균 문제점 전체 corpus(문장, document)에 대해서 loss와 grandient를 한번에 계산하는 것은 너무 비쌈.. 메모리 문제.. 그래서 Stochastic Gradient Descent 기억하니??? 이러한 작은 data chunk에 대해서 loss와 gradient를 계산하고 update..\n이걸 적용해보겠다\u0026hellip; 그래서 한 문장에 대해서 loss를 계산하고(원래는 여러 문장 batch에 대해서 계산했었음) graeidnt랑 weight를 업데이트 해서.. 이제 다른 새로운 한 문장에 대해서 반복한다~ Backpropagation for RNNs (W parameter를 훈련 시키기 위함!) [##Image|kage@mg3iq/btsOK3gBiFm/AAAAAAAAAAAAAAAAAAAAAOoIiVeMS4EAFLxLbEQdFCnjBPX6LasLXf1C3cmF95GE/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=pAazc3X5vbWcsh65DO1koqUo68s%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:756,\u0026ldquo;originHeight\u0026rdquo;:419,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:612,\u0026ldquo;height\u0026rdquo;:339}##]\n가중치 matrix Wh에 대해서 Jt의 미분은 무엇인가? 여러 위치에서 반복적으로 사용된 같은 가중치에 대한 gradient는 각각의 사용 위치에서 나온 gradient들을 모두 더한것! 즉, 각 timestep에서 loss에 기여한 W에 gradient가 따로 생김! 이 W에 대해 업데이트 하려면 모든 timestep에서 나온 gradient를 한번에 더해야해! [##Image|kage@3lKBN/btsOLfnQEAb/AAAAAAAAAAAAAAAAAAAAAPjbKuP46Dg-E-VCUPX_CFF7mHCsyRNM6Q1Is8H4ZAvF/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=u%2BO2hvqDSwJXiSFX%2Bq0GuR%2FeUeU%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:872,\u0026ldquo;originHeight\u0026rdquo;:459,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\ntimestep를 따라 거꾸로 역전파하면서(t, t-1, .. 0) 각 시점마다의 gradient를 계산해서 전부 더한다. 따라서 backpropatgation through time (BPTT) 그니까 시간을 거꾸로 돌면서\u0026hellip;! 계산한다! 마치 n-gram LM같음! 반복되는 sampling을 통해서 text를 생성하는~~\nProblems with RNNs: Vanishing and Exploding Gradients 그러면 이제 각 timestep t에 대해서 Wh에 대한 J4의 gradient를 계산해야 함! 이때 시점이 1일 때 J4/Wh\n[##Image|kage@c6Mtp5/btsOMqIBmyj/AAAAAAAAAAAAAAAAAAAAAKBM8pAVxyGZw73TMjsAJ0pmdqhp4Zt-xEYITKJG6djN/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=3BiU%2FM%2BvC7sZD6aur%2BHCTsfdFps%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:595,\u0026ldquo;originHeight\u0026rdquo;:350,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n우리가 4번째 Loss를 계산하고 이것을 첫 번째 W(hidden state)에 대한 가중치를 계산해야 함! [##Image|kage@c41Eec/btsOKq4vd6Y/AAAAAAAAAAAAAAAAAAAAAJdDrLhfcBcbPxKpllFz0izw43toU0HB8hVXWQxP56yA/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=e7hJ%2BvM10%2FBDDhCsVZL1ldD2sT0%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:799,\u0026ldquo;originHeight\u0026rdquo;:511,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:458,\u0026ldquo;height\u0026rdquo;:293}##]\n이를 위해서는 chain rule에 따라 위와 같이 계산된다.. 그런데\u0026hellip; 곱해지는 저 값들이 작으면 어떻게 되니? Vanishing gradient problem\nbackpropagation이 더 멀리 진행될 수록 gradient signal이 더 작이지고 작아지는 현상\n[##Image|kage@cSSAqq/btsOLwJyfLB/AAAAAAAAAAAAAAAAAAAAANx0D3CcClzEqW80cTkfeIK2NffZKwAtp-HYRQzx45o5/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=MM5pOJ%2Fsdg7lqRrGlRThWq9NhhQ%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:770,\u0026ldquo;originHeight\u0026rdquo;:409,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n그러니까\u0026hellip; ht 함수에는 wh 라는 우리가 계산해야 할 가중치가 있고 또 그 안의 ht-1 함수에는 우리가 계산해야 할 가중치가 있다. chain으로 곱채지는데\u0026hellip; 곱해지는 값이 작은 수면.. 작은 수 끼리 계속 곱해진다\u0026hellip; 따라서 지수적으로 작아진다는 문제가 발생\n[##Image|kage@UOpMn/btsOLdDCn8y/AAAAAAAAAAAAAAAAAAAAAJ1SE3Q95f-Db_u_s_neu4yQdaqc7_iRYuoKmdU0ES1p/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=pPPO1mCmNROOkzinLjcn9Q5Z7Bs%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:916,\u0026ldquo;originHeight\u0026rdquo;:455,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n그래서 문제는\u0026hellip; backpropagation을 할 때 timestep이 올라갈 수록 점점 그 값이 작아져서 .. 나랑 가까운 시점의 gradient는 크고 나랑 먼 시점의 gradient는 기억하지 못한다는 단점이 있다. 즉, 짧은 기억만 가능하다!!! 긴 문장 처리 못 함!! 즉 어떤 t 시점의 Loss와 W를 계산하려면 i 시점의 gradient 즉, Loss t / hi를 계산해야 하고 이때 i가 t보다 너무 멀면 아예 값이 작아져서 영향을 못 준다는 거구나!!!\n그래서\u0026hellip; 긴 문장이 주어지고 빈칸을 주어지는 문제를 맞출 때\u0026hellip;! When she tried to print her tickets, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her ________\n여기서 답은 ticket 즉, 맨 앞의 7번째 단어를 보고 맞춰야 하는데.. 너무 멀어서 vanishing 되기 때문에.. 못 맞춤.. exploding 문제는 걍.. 아예 다른 값이 나올 수 있다는 뜻!!! NaN\n이 문제는 Gradient가 일정 threshold보다 크면 잘라주는 작업을 하게 됨! clipping!! Vanishing 문제를 어떻게 해결해야 할까? 1. 기억을 따로 보관하고 더해가는 RNN은 어때?\u0026quot;\n-\u0026gt; LSTM\n2. 정보가 더 직접적으로 흘러가게 (linear pass-through) 하는 구조\n-\u0026gt; Attention이나 Residual Connection\n추가 질문 Q. 여기서 embedding은 word2vector와 같은 pretrained embedding model을 통해 를 통해서 embedding 되는건가?\nNo! 그냥 가중치 W에 의해 vector화 해주는거고 pretraining 단계에서 embedding은 자동으로 함께 학습된다. (즉 end-to-end로 같이 학습) Q. RNN은 LM인가?\n아님!! RNN은 모델 아키텍쳐이고 LM은 학습 목적이자 Task!!! 즉 RNN은 시퀀스를 처리하는 신경망 구조이고 LM은 텍스트에서 다음 단어 예측 같은 Task를 수행하는 모델 너는 RNN을 써서 Language Model을 만들 수 있어\n→ 예: RNN-based Language Model (2015년 이전에 많이 사용) 하지만 RNN이 항상 Language Model을 의미하는 건 아니야\n→ 예: RNN으로 음악 생성, 시계열 예측, 번역 등 비-LM task도 가능함 그리고 Language Model이 꼭 RNN을 써야 하는 것도 아냐\n→ GPT 같은 모델은 Transformer 기반 LM ","date":"2025-06-21T16:53:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/recurrent-neural-networksrnn/","title":"Recurrent Neural Networks(RNN)"},{"content":"Language Modeling의 정의 다음에 올 단어를 예측하는 task를 수행한다\nEX) the students opened their ____ -\u0026gt; (books, laptops, exams, minds)\n주어진 단어들의 sequence(== 문장)\nx1, x2, ,,,, xt\n다음 단어 xt+1의 probability distribution를 계산하는 것 x+1은 vocabulary V= {w1, \u0026hellip; , wV}의 아무 단어를 의미한다. 위와 같은 작업을 수행하는 것이 Language Model이다. 쉽게 생각해서 단더 조각의 probabiltiy를 할당해주는 system이라고 생각하자! [##Image|kage@bEu4UG/btsOMbx4VHd/AAAAAAAAAAAAAAAAAAAAALAuedCmtWhEmgVvyRDKLlYNoK0C_VgaLV7x_pUgvez8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=c1L%2Ft7T%2BP3fKyowIDA7ity6N4xU%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:628,\u0026ldquo;originHeight\u0026rdquo;:179,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n그러니까 즉, 어떤 문장이 생성되기 위해서는 첫 번째 단어가 올 확률 x 첫 번째 단어가 있을 때 2번째 단어가 올 확률 x \u0026hellip;\n쉽게 생각하면 input은 그동안 생성한 글의 확률, output은 그 글의 다음 단어의 확률 Language Modeling을 왜 신경써야 할까? Language Modeling는 benchmark task이다!\n이는 우리의 progress가 잘 예측하는지 측정하는데 도움이 된다. 또한 많은 NLP(Natural Language Process)의 subcomponent라고 보면 된다.\n왜냐하면 text 생성 이나 text의 확률을 예측하는 일에서 매우 중요하다ㅏ. • Predictive typing\n• Speech recognition\n• Handwriting recognition\n• Spelling/grammar correction\n• Authorship identification\n• Machine translation\n• Summarization\n• Dialogue\n결론\nNLP의 모든 기술은 LM을 기반으로 다시 설계된 것.\n옛날에는 NLP 각 task마다 model이 따로 존재(감정 분석: SVM, 번역: RNN, 요약 Seq2Seq\u0026hellip;)\n요즘은 대규모 언어 모델(GPT, Bert) 하나로 모든 걸 해결! Next word prediction [##Image|kage@B880X/btsOK56w4m9/AAAAAAAAAAAAAAAAAAAAAGi7MTKKVksaHyg8oGF0rKmbkx07-XeOS6WYhS79W1rJ/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=ZvaUY0HkP726Ec1gIbbi2I5wAXM%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:876,\u0026ldquo;originHeight\u0026rdquo;:272,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:699,\u0026ldquo;height\u0026rdquo;:217}##]\n이런식으로 문장 안의 blank를 두어서 예측을 하도록 학습시켰다면\u0026hellip; [##Image|kage@brGrYk/btsOMbrhW2U/AAAAAAAAAAAAAAAAAAAAAJVZ7cxHL1j8l05ObJ55ytt-g0YBZVYRgco6NaEaiMlP/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=I736lUxcPUJPfuxX3GzgIH38w3U%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:768,\u0026ldquo;originHeight\u0026rdquo;:364,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;,\u0026ldquo;width\u0026rdquo;:684,\u0026ldquo;height\u0026rdquo;:324,\u0026ldquo;caption\u0026rdquo;:\u0026ldquo;Context와 example 하나를 보여주고 질문에 답을 하게 한다.\u0026rdquo;}##]\n요즘은 이렇게 GPT 같은 모델들이 in-context learning 으로 작업을 수행하게 한다. 즉, 몇 개의 example을 주어지면 그 작업을 수행하게 한다. fine-tuning(훈련)하지 않고 propmt 상에서 example 몇 개(한 개 혹은 그 이상)만 보여주어서 새로운 작업을 수행하게 한다. 예전에는 어떻게 Language Model을 학습시켰을까? n-gram Language Model!\nMarkov assumption\n현재 상태는 오직 직전 상태에 의존한다. 과거의 모든 정보는 직전 상태(n이면 n-1개까지의 과거)에 요약되어 있다! 자연어 처리에서는\u0026hellip; [##Image|kage@buCTf6/btsOLCCYRPY/AAAAAAAAAAAAAAAAAAAAAEJfukJ_v1O9xuY3QzTuz7i_7f-TsttY1Bv7BWikBI8k/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=xhloXgei4u44u82gqR%2B99W%2FZtIk%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:762,\u0026ldquo;originHeight\u0026rdquo;:114,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\nxt+1(현재 단어)은 이전의 n-1 개의 단어에(과거의 정보) 의존한다.\n[##Image|kage@cpa9dj/btsOKBkD6Gh/AAAAAAAAAAAAAAAAAAAAAE1v60qR30vxs8aj35g8PCBZX72lX-Cy40UCZ6SEp8ps/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=qIY0LUvEpKo1VTag8E7SzXTM%2Feo%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:614,\u0026ldquo;originHeight\u0026rdquo;:187,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n만약 4-gram Language Model이라고 하면 3개의 단어를 확인 이를 직접 statistical approximation 하게 count 한다. [##Image|kage@MjTJo/btsOLvqmrvI/AAAAAAAAAAAAAAAAAAAAAPGTzzs7bDTPGiHL-aOJuJUES5godmZjlxljy8xrcZvy/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=9ArLFBmwn2JsNsaf4ukucZ4IMwA%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:730,\u0026ldquo;originHeight\u0026rdquo;:202,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n즉 corpus에서 3개의 단어가 1000개 나왔을 때 다음 단어를 count해서 학습 하지만 proctor 라는 context(문맥)을 아예 무시해도 될까??\n문제 1 만약 data에 \u0026ldquo;students opened their w\u0026rdquo; 이 아예 나타나지 않으면? -\u0026gt; 작은 입실론을 추가해준다. 문제 2 \u0026ldquo;students opened their\u0026rdquo; 가 아예 없으면?\n-\u0026gt; backoff로 \u0026ldquo;opened their\u0026rdquo; 로 대신해라! 문제 3 그리고 그 모든 확률을 다 count 해야 하는데..? -\u0026gt; storage 문제 발생 사실 n=5이상이면 문제가 발생해서\u0026hellip; 잘 안 쓴다. 근데\u0026hellip;\nincohrent하다!! 즉 앞뒤가 안 맞는 문장이 생성된다. 적어도 3개 이상의 단어를 고려해야 하는데\u0026hellip; n이 커지면 위와 같은 문제가 발생하고\u0026hellip; 어뜩하지 Neural Language Model? 즉 확률을 모델링하는 것이 아니라\u0026hellip; Deep Learning(신경망)을 이용해서 확률을 모델링하자! [##Image|kage@UZADO/btsOKDicr3a/AAAAAAAAAAAAAAAAAAAAAKs73WHd8Jz4kU62qvLWkSTEREbXI-zd3iVBIIyhRW8t/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8\u0026amp;expires=1761922799\u0026amp;allow_ip=\u0026amp;allow_referer=\u0026amp;signature=bLSWe3euV220bNV8vjgc99woGa4%3D|CDM|1.3|{\u0026ldquo;originWidth\u0026rdquo;:721,\u0026ldquo;originHeight\u0026rdquo;:495,\u0026ldquo;style\u0026rdquo;:\u0026ldquo;alignCenter\u0026rdquo;}##]\n즉 위와 같이 vector를 만들어서 embedding을 해주고 hidden layer를 통해 계산. 따라서 n-gram LM의 문제를 해결한다! 관찰되어야 할 n-gram을 모두 저장할 필요가 없고 정보가 존재하는지 아닌지도 문제 (sparsity problem) 도 없어진다!\n그러나\u0026hellip; 다음과 같은 한계가 존재하는데\u0026hellip;.\nFixed window도 너무 작다. window size를 키우면 W(가중치)도 더 커진다!!! 즉, 매우 긴 sequnce를 이해할 수 없다!! (문장 앞부분의 개념이 뒷부분 해석에 중요할 수도 있으니!) x1과 x2는 곱해지는 가중치가 다르다! (즉, 입력 위치에 따라 처리 방식이 달라진다) 이런 경우 입력 위치가 다르면 같은 단어라도 다르게 처리될 수도..(즉 dog라는 의미는 앞이든 뒤이든 같아야 함!!) 어떤 길이의 input이든 처리할 수 있는 neural architecture가 필요하다!!! 이제\u0026hellip; RNN 을 배울 때다!!\n","date":"2025-06-21T14:10:00+09:00","permalink":"https://juijeong8324.github.io/p/nlp/language-modeling/","title":"Language Modeling"}]